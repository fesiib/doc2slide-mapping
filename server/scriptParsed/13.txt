- Yeah, hi my name is Prem.
This is a joint work with Adobe and Northwestern University, and the paper's called VoiceAssist: Guiding Users to High-Quality Voice Recordings.
So, if you're in this room, you've probably seen something like this in the recent past possibly while editing the videos for this conference, for example.
And basically what we're trying to do is tryna make a narration on top of some video or just narration by it's self for like a radio program or something like that.
Right, and you're faced with an interface that looks like this.
There's record buttons, play buttons.
There's like a multi track kind of view.
Another version of this one would be a popular program called Audacity where you have a similar kind of view or you're just recording.
And the hope is that when you sit down to record something, it sounds like this.
- [Man] So, in 1853 during the California Gold Rush, a leafleteer out West published the 10 Commandments for Gold Miners who'd come out to prospect.
- Right, the hope is that you sound perfect, like Ira Glass.
You know, the perfect kind of radio voice.
Everything's very present and it sounds really, really great.
What actually ends up happening is something more like this.
- [Man] Unfortunately our recordings usually sound like this with too much background noise and reverb.
- We end up recording in sub-optimal places such as this room, for example.
You can probably hear a lot of reverb of my voice right now.
(laughing) and we end up using our microphones in suboptimal ways maybe they are pointed in the wrong direction.
Maybe we're recording in a corner.
Maybe we haven't treated the room properly to reduce the number of reverb.
The number of reverberation and stuff like that.
So, we set out to try to make an interface that kind of told users when these issues were happening and provided them with the visual feed back that indicated that.
And let them react in real time to that visual feed back to produce a better recording.
SO this is the interface right here.
This is what we call voice assist.
On the left there's a room acoustics box and on the right there's a background noise box.
And I will go into each of these.
What goes into each of these boxes.
The idea is that the more red that a box is, the worse that the machine thinks your recording is.
The greener that a box is, the better that you're doing for that recording.
So, in each of these boxes are two measures.
On the left for room acoustics, we use this thing called the speech transition index.
So, the speech transition index is something that ranges between zero and one.
So .95 and above is NPR level audio quality.
That's Ira Glass levels, right? Around .85 is what you'd experience in a normal kind of room like a bedroom or an office where you might be recording.
Around .5 would be a parking garage.
I would say this room with this microphone is around .6.
All right, so, in prior work we developed a way to measure this using a deep neural network.
It takes an audio sample and out puts a score between zero and one.
And, this work is about integrating that into a user interface.
- [Ira] So in 1853.
- Oh, sorry, so, I'm gonna play a bunch of recordings right now that are at different levels of the speech transition index.
It's supposed to be, like, the first one's gonna be at one and then it's gonna keep on going down.
Because of the reverb in the room the first one's actually gonna be at like, gonna sound to you as if it's at .8 not at one.
So here's the first one.
- [Ira] So In 1853 during The California gold rush, a leafleteer out West would publish The Ten Commandments for Gold Miners who'd come out to prospect.
- So the next level down.
- [Ira] So in 1853 during the California Gold Rush, a leafleteer out West would publish The Ten Commandments for Gold Miners who'd come out to prospect.
So in 1853 during the California Gold Rush, a leafleteer out West would publish The Ten Commandments for Gold Miners who'd come out to prospect.
So in 1853 during the California Gold Rush, a leafleteer would publish The Ten Commandments for Gold Miners who'd come out to prospect.
- All right, so his voice is getting more and more distant.
There are lot more echos in the room and things like that, and part of what can affect that is basically distance from the microphone.
So if I'm talking here and I end up coming back here, you might, you hear a different kind of sound coming out of it.
Yeah.
So the other box is just signal to noise ratio, so this is kind of used as a more traditional signal processing approach.
So the idea is that if I'm speaking, there's some background noise happening, right? So, maybe a fan on in the background or air conditioning or something along those lines.
When I have something like that going on in the background, if it's too loud it kind of makes the audio more unintelligible.
So, a signal to noise ratio just basically compares how loud the speech is to how loud the background noise is.
So, like an NPR level audio, like will have around 40 dp of, like, boost from the recorded speech to the background noise.
So, I'm gonna play again a bunch of examples.
I will play the perfect one first and then with increasing levels of, or yeah decreasing signal to noise ratio.
- [Ira] So in 1853 during the California Gold Rush, a leafleteer out West published The Ten Commandments for Gold Miners who'd come out to prospect.
- So this one's, the next one's gonna be just a little bit worse.
- [Ira] So in 1853 during the California Gold Rush, a leefleteer out West published The Ten Commandments for Gold Miners who'd come out to prospect.
- You might not notice that one over the noise in this room right now.
But if you had headphones on you would, it would not be great.
Here's one that's much worse now.
- [Ira] So, in 1853 during the California Gold Rush, a leefleteer out West published The Ten Commandments for Gold Miners who'd come out to prospect.
- So, again, those two boxes are kind of in the centerface, right? So on the left the speech transition index kind of being computed by a deep neural network on the fly and on the right is signal to noise ratio that's also being computed on the fly through more traditional signal processing.
So, the color of the box indicates the quality so in this instance, on the left, I have poor audio quality, or poor room acoustics right? And on the right I have the signal to noise ratio is pretty good.
So, I'm gonna play just a very short, like video of me kind of talking and you'll hear, you'll see the boxes kind of react to what's happening to the audio quality in real time.
If a user moves away from the microphone and there's too much background noise, the quality indicators become red indicating that the user should make adjustments to improve audio quality.
Something strange happened with the volume there.
But at the beginning I was kinda talking off mic a little further away.
There was a lot more reverb and kind of back ground noise in it.
And then I moved closer to the mic which kind of increases the signal to noise ratio and because I'm closer to the microphone, there's less reverb coming in from off mic sources like walls and things like that.
So, the simplicity of the interface which is just boxes kind of come from some design integration that we did.
So here, this is an early prototype of an early design for voice assist.
So in this kind of thing on the left, we had recording quality that kind of flowed in like right to left.
You know an it kind of, the higher, the more filled the box was the better it was.
And then there was also this check box that kind of said like, check or X or something like that.
And then some sounds great.
You know it tries to give some advice on what to do.
What we found in the studies is that this design had way too much cognitive load for the task that we wanted to do.
So when people were trying to record their excerpts and things like that, they wanted to focus on the task at hand and they wanted an interface that they could kind of keep track of in their peripheral vision but not have, that doesn't call too much attention to itself because they found it distracting.
So, with that kind of colored box interface we wanted to evaluate it so, our experiment sought to answer three questions, so one is, do objective quality measures like signal to noise ratio and speech transition index.
Do these things improve significantly when a user uses voice assist? Do the users themselves believe that they made a better recording with the voice assist versus without it? And do third party listeners when they listen to recordings that are made with voice assist, do they prefer those to ones made without voice assist? So, we had two interfaces that we were testing.
First one was just a base line interface that you might see in Audacity.
Just kind of a mock up version of the audacity interface where you just got a record button and you get an amplitude kind of thing.
So, again, it's a time thing so it flows from right to left and it just kind of tells you how loud you are at any instance.
So this is the base line interface.
This is the one we are comparing against.
The other interface, of course, is voice assist which in addition to the base line interface also gives these colored boxes that tell the user about the audio quality.
So, each participant in our user study makes two recordings.
So the first recording they make is with the baseline interface.
For every participant.
So they're just given the baseline interface, they are given a standard text that every one records, and they just have to speak that text.
Then they make second recording and if they're in a control condition they just use the baseline interface again.
If they're in a test condition they use voice assist.
The reason for that is because you record something multiple times you might just naturally get better at it.
So you wanted to see you record something twice but the second times with a different interface to try to disambiguate those two cases from each other.
So the initial set up for recording is their in an environment where high recording quality is possible.
Right, so they can get to a high-quality voice recording.
In the room they use, there's a popular microphone for podcasting called the Blue Yeti.
You search podcast microphone on Amazon it will almost certainly be the top result.
It's what everyone buys when they're starting out for podcasting and things like that.
However, in the room the microphones pointed in the wrong direction, so if you speaking it, if you speak without adjusting your set up you'll get a bad recording.
It's placed far away, it's kind of off access and then the gain in the position of the microphone has to be adjusted.
So they have to move it around and they have to play with the gain a little bit to get optimal audio quality.
So, for our user study we recruited 23 participants.
12 men, 11 women.
Six men, six women were in the test condition and six men, five women were in the control condition.
It's important that it was gender balanced because the underlying to signal processing and machine learning tools can respond differently to different frequencies, so we wanted to make sure that it worked for everyone.
So these are the instructions.
This is kind of the work flow that they went through.
So first, they were given some time to familiarize themselves with the text that they needed to record.
Then, they recorded the text with the base line interface.
Then they listened to that entire recording and they rated it's quality.
All right, so they got to listen to the recording they just made and rate how good it was, and then they experimented with the second interface.
Right, so if it was the control condition they were given instructions like hey you can adjust your environment.
You can move your microphone around and things like that to try and find a better recording for the second take.
And if it was voice assist they could, you know, experiment with it and see, okay, let's try to make these boxes green or red or you know what ever they wanted to do.
And so they're given kind of an unlimited amount of training time.
So they are just aloud to train for as long as they want.
Play around with the environment, try to get to a good recording.
Then they record the same text again and then they listen to the second recording and they rate that recordings quality.
So, I will play now a couple of examples from the user study.
So this is the control condition that I'm going to go through first.
SO the first recording a user makes sounds like this.
- [Man] The appearance of the island when I came on deck next morning was altogether changed.
- Sorry can we turn it up a little bit? I'll play it one more time.
These are all a little bit quieter, so.
- [Man] The appearance of the island when I came on deck next morning was altogether changed.
- All right, so you can hear there's some room tone, there's a little bit of background noise on that recording.
Here's the second recording.
So again, they were given time to experiment with it and they were able to adjust.
We gave them instructions on maybe how to adjust and things like that.
And this is the second recording.
Given a lot of help to maybe make a better recording.
- [Man] The appearance of the island when I came on deck the next morning was altogether changed.
- There's still a ton of room tone in that recording.
There's still a lot of background noise in the recording, and it's almost, except for the volume difference that just happened, that it's almost indistinguishable from the other recordings.
So nothing has really nothing really improved in the control condition for this pair.
So, in contrast here is a recording that happened in a test condition.
So here's the before.
- [Woman] The appearance of the island when I came on deck next morning was altogether changed.
- So that's the before, here's what happened.
I'm gonna play like a short time lapse, so this is what happened during the training period.
So you can see this is the room.
It's kind of like a, it's a recording booth.
There's the microphone right there, the silver thing.
And she's kind of positioned away from it.
So during training she kind of fiddles around, she moves the microphone around to get a better recording.
And the knobs on the back on the microphone there are gain, pick up pattern.
Okay, so here's what the training looked like.
So she's trying to make the boxes green.
She's kind of experimenting with the gain right now.
She finally figures out, yeah, the microphone is pointed in the wrong direction.
And then finds a better kind of position.
Here's what the second recording sounds like.
- [Woman] The appearance of the island when I came back on deck the next morning was altogether changed.
- So there's a lot less room tone in that recording.
There's not a lot of back ground noise in the recording.
So we studied the impact first on audio quality measures so we looked at the speed transition index in each pair of recordings.
So we're comparing a recording that was made in the control condition the first, like the first recording to the second recording.
So every comparison is done with in user.
There's no cross user comparison here.
So, with in users we compare first to second.
We measure the audio quality measure along these two lines and we see if we detect a change.
So in the control case we don't detect any change.
The speech transition index and the signal to noise ratio stays basically the same from within the pair.
However, in the test case, which uses voice assist we saw there was a significant change.
So the speech transition index increased by about .03 and the signal to noise ratio increased by 2.3 db.
And one thing to note about this .03 number is that we are dealing with kind of a very smaller range of you know where acceptable, good audio quality is.
The difference between .92 and .95 is essentially the difference between like, maybe, a good bedroom recording versus an NPR recording level studio.
And in a separate study that somebody did a long time ago, like .03 is actually the just noticeable difference for speech transition index.
So in the self-reported results, we had noticed that users did not report that the second recording was significantly better than the first recording.
All right, there might be a few reasons for this you know they might not be listening critically to their own recording.
They might not of gone back and compared effectively between the two recordings.
Or they might not have good idea of what a good recording sounds like.
However, when we presented those pairs to others, to third party listeners, we used a crowd source audio quality evaluation called Cake, it's an open source software for comparing pairs of recordings and doing kind of subjective listening tests on like Amazon mechanical turk.
So we recruited a bunch of mechanical turkers and we asked them, hey, here's a pair of recordings, first recording, second recording, which one do you prefer? All right, so in the control condition we noticed that there was basically no preference.
There's 61 that preferred the first recording.
62 preferred the second recording.
However, in the test condition, which is the one that uses voice assist, we noticed that there was big difference.
One, the first recording, people only preferred 30 versus 96 on the second recording.
So in conclusion, we found that voice assist is an effective interface for improving audio quality recordings.
The objective analysis, the speech transition index, and signal to noise ration improved between the two recordings when you used voice assist.
Third party listeners preferred voice assist recordings but the users themselves didn't really notice any difference.
And, that's my talk, thank you.
(audience applauding) - [Woman] We have time for a few questions.
- [Chelsea] Fantastic presentation.
My name is Chelsea Myers, I'm from Drexel University.
So, my question is, how much did your participants learn to edit their recording through voice assist instructions versus your instructions that you provided? I'm really interested to hear a little bit more about that.
The recording, the video recording, that we have, like what was the process looking like there with oh the gain, and this and that? - Right, so we kept the kinda of instructions a little bit vague so these are the instructions, these are the only instructions they really got.
So it was to adjust move closer or adjust re-orient the microphone.
It doesn't tell them what direction to re-orient it.
Partly because there's no way to, like, say that outright in any use case.
Like, you do this or that, right? 'Cause you know, we don't know how it's gonna be, if we deploy this, which way their microphone is going to be pointing.
So giving kind of specific advice can be more difficult.
It's definitely a topic of future work that we're doing stuff.
And they got the same instructions in the base line condition.
What we noticed, if you look at the videos, is that people just didn't experiment in the control condition at all.
Like, these are people that we brought in, too, so they were definitely participating in the test but they really just didn't know what to do, they just kind of fiddled around the environment.
But they didn't know what direction to go in or how to alter things to make things sound better and stuff like that.
- [Chelsea] So was the woman in the video kind of looking at these colors and-- - Yeah.
- [Chelsea] Okay, all Right.
- Yeah she's looking down at a lap top while she's playing around with the environment and stuff like that.
- [Chelsea] Awesome, thank you! - Unfortunately, we need to switch to the next speaker.
The speakers will be here after the talks so you can come down to the front of the room and speak to him afterwards.
