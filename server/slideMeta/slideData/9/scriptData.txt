
 - Hello everyone. So I'm gonna present a set of experiments that show how our traditional notions of separability, essential tendon in visualization design falls short, and how we can fix them.
 So consider this simple scatter plot where I've encoded two data dimensions using position. We can encode additional data into this scatter plot using visual channels such as color, size, and shape. But how do these visual channels interact with one another perceptually? For example, if I encode one data dimension using shape, will it change the way I perceive color or size differences between marks? So this work models the perceptual interplay between these channels. And using these models, we can anticipate and account for potential errors that people might make when interpreting multi-variant visualizations.
 So we can encoded data dimensions using various visual channels, such as position, size, shape, and others.
 of separable versus integral channels. So separable channels is when one channel can be attended to without any interference from the other. So for example, if I encode data using position as the first channel, and encode additional data using color as the second channel, I can determine the position of these marks, no matter what color they are. And I can determine the color of these marks, no matter where they are in the scatter plot. So there's no interference.
 Conversely, integral channels is when encoding a data attribute using a visual channel interferes with selectively attending to the other. For example, if I encode data using lightness as the first channel,
 So here I have an ordered list of visual channel pairs where pairs at the top are known to be more integral, and pairs at the bottom are known to be more separable. But the specific ordering of this list is primarily derived from intuition and experience with limited empirical support. And these heuristics have been widely used by visualization designers, but their actual utility is poorly grounded, leaving users open to potentially significant data interpretation errors.
 So we aim to provide designers with more concrete guidelines about reasoning, about separability. And we do this by empirically measuring the amount of separability between channels, and then constructing actionable models that we can use to support visualization design by automatically adjusting encodings to account for perceptual interference.
 We constructed these models using data collected from 1,930 participants on Mechanical Turk, across four different experiments. And I'm gonna discuss two of these experiments in detail, and I'll also discuss how we can use the results from these experiments to craft more effective multi-variant visualizations. Okay, so getting into the experimental design,
 here's an example stimuli from one of our experiments. And for each data plot, participants were asked to compare two target marks and indicate whether they were the same or different on some specified dimension.
 We tested 16 different shapes gathered from common vis tools like Tablo, and MATLAB, and Python, and others. And these shapes can be broken up into three major categories: filled, unfilled, and open shapes.
 We tested six different mark sizes ranging from six pixels to 50 pixels in diameter based on those used from prior experiments in this area.
 And we tested a variety of colors that differ along the lightness axis, the red-green axis, and the yellow-blue axis in the CIELAB color space.
 CIELAB is commonly used in visualizations because it's approximately perceptually uniform, meaning one unit in Euclidean distance, equates to one just noticeable difference, or JND for short. And we define a JND as the minimal amount of distance that two colors need to be apart for 50% of people to actually perceive a difference. Now this point about one unit of Euclidean distance equating to one JND, actually doesn't hold in practice, because CIELAB assumes specific conditions for comparing colors, like very large isolated marks. And that's just often not the case in visualizations.
 For example here, I've encoded a scatter plot using seven different colors that are each separated by one Euclidean distance in this color space. Theoretically we should be able to discern small differences in these colors that would equate to small differences in the data. But as you can see, it's hard to determine these differences, it's hard to discern these color differences from marks of this shape and size. This means any meaningful small scale differences in the data that these colors in code are lost.
 So drawing from prior work and visualization and vision science. We derived three general hypotheses related to these experiments. First, we expect colors to be more discriminable on denser shapes. And this hypothesis stems from prior experiments that have shown that color discriminability is proportional to mark size.
 Secondly we expect denser shapes to appear larger. And this hypothesis stems from the idea that when comparing the size of two marks, users might resort to other aspects of the marks, such as density, which could alter perceived size.
 between these channels to be asymmetric. In other words, we expect shape to more strongly influence color and size perceptions. Then either color or size would influence one's ability to perceive a marked shape.
 Okay, so let's get into the first experiment, which explores interactions between shape and color.
 So this experiment specifically looks at how using different shapes in a scatter plot might affect color difference perception. Here's an example stimuli from the experiment. And the scatter plot consists of a field of gray distractor shapes with two color shapes that are either the same color or slightly different colors. And we asked participants to indicate whether these colored marks were the same or different colors, which allows us to model JND values for different marks, sizes, and shapes.
 We used a full factorial mixed factors design for this experiment where we tested 16 different shapes,
 We tested all three axes at CIELAB between.
 And we recruited 606 participants for this study. And if I didn't mention it before, we're using Amazon Mechanical Turk.
 So getting into the results, what we're looking at here is color discriminability results associated with three of our 16 tested shapes. Where the X axis denotes mark size and pixels, and the Y axis shows us the mean 50% JND for lightness differences. And we're gonna focus on lightness for the purpose of this talk because lightness is the most important aspect of color discriminability for numerical encodings. And to remind you, a JND is the minimal amount of distance that two colors need to be apart such that that difference is detected half the time. So looking at this graph, we can see that colors become more discriminable as mark size increases, because a lower value means more discriminable. But we can also see that depending on the mark's shape, the JND values are different. For example, we see a 50% reduction in discriminability for six pixel Y crossings, compared with filled squares, suggesting that the shape of a mark also influences color discriminability.
 So the primary takeaways from this first experiment are that shape does significantly affect color difference perception, where, for example, the filled diamond had a mean JND of about 6 1/2, while the Y crossing had one of over 10 which is a substantial difference. And also, this experiment replicates results from prior studies that show that color discriminability is proportional to mark size.
 So our first experiment showed that color discriminability varies with shape. And we hypothesized that this variation may be explained by a variation in perceived size. So shapes that are perceived as larger, might be more discriminable. So we ran another experiment to test this hypothesis that looks at shape and size.
 Participants in this experiment saw a scatter plot like this with two target marks highlighted in blue. And they were asked to determine which of the two highlighted shapes appears larger.
 We again, used the full factorial design, where each participant completed 123 trials. And we tested the same set of shapes and sizes from the previous experiment.
 We recruited 489 participants for this study.
 So this graph summarizes the results of our study using the mean size bias for each tested shape. The X axis here shows the percentage of the time that a particular shape was perceived as larger than other shapes of the exact same size. So if there was no bias due to shape, we would expect these values to hover around 50%. But we see significant variation from this threshold. Now if we look at the mean size bias for the filled shapes and compare that to the unfilled shapes, we generally see that the filled shapes are perceived as larger than their unfilled counterparts. For example, the filled circle is perceived as larger in 67% of trials, while the unfilled circle is only perceived as larger in 49% of trials. So this pattern supports our hypothesis that denser shapes would be perceived as larger. But we don't see the same pattern in open shapes. For example, the T crossing is seen as larger in 82% of trials, while the denser 8 prong asterisk is seen as larger in only 54% of trials. And overall we see no evidence that denser shapes, denser open shapes appear larger. Also we found that the patterns in perceived size don't well explain the patterns in color discriminability that we saw from experiment one, suggesting something more complex is at play here.
 So the main takeaways from this second experiment is first that shape does significantly affect size perception where some shapes are perceived as larger more than 80% of the time when compared to other shapes of the same size. And color discriminability for our tested shapes is not well explained by the perceived size of the shape.
 So we also ran a pair of experiments that explore the symmetry of these relationships. And we care about symmetry because if perceptional interference functions asymmetrically, then we can adjust encodings to account for interference in one direction without worrying about it altering perceptions in the other direction. But for the sake of time, I'm not gonna go into detail about these experiments. So I'll just tell you that we found that color and size have much less of an effect on shape perception than we have seen shape have on size and color perception.
 So lastly, I wanna close by summarizing our results and showing how we might use this data to construct more affective visualizations.
 that colors would be more discriminable on denser shapes like with the filled shapes compared to the unfilled shapes. But we also found evidence contrary to this hypothesis among the open shapes.
 And we found some support for our hypothesis that denser shapes would appear larger. For example, the filled shapes compared with their unfilled counterparts. But again we found some evidence contrary to this hypothesis. And in general, the perceived size of a shape didn't well explain the color discriminability results from the first experiment.
 that shows an asymmetry between these channels. Shape had a much stronger affect on color and size perceptions than color or size had on shape perceptions. But again, see the paper for more details on these experiments.
 Okay, so we can use the models constructed from the results to build more effective multi-variant visualizations. And if we revisit our example earlier from the talk that used one Euclidean distance JNDs, which is standard in CIELAB. We lost differences in our data, because we should, theoretically, be able to see seven values here. But we can instead adjust to these color steps based on the modeled perceptual interference between shape, size, and color, and compute new JND values.
 So future directions for this work include running additional experiments to better understand the precise perceptual mechanisms at play here with shape, size, and color. Because our results suggest that there are a number of perceptual factors that determine how a shape influences size and color perceptions. And we wanna borrow from techniques used in vision science to better disentangle these factors and generate a more generalizable set of intuitions about shape separability. Also, simultaneous contrast effects can affect color perception, and to some extent, may affect size perceptions as well. So we'd like to study how separability between these channels might change using different background colors.
 So the main takeaways from this work are that shape does significantly affect color difference perception and significantly affect size perception.
 allowing us to adjust encodings for perceptual interference in one direction, without worrying about altering perceptions in the other.
 is a little less understood than we previously thought, but separability can be modeled. And these models can be used to automatically adjust encodings, and improve visualization design, and thus increase interpretation accuracy.
 So thank you all for listening to this talk. And thanks to my advisor, Danielle Safer for her support on this project. And thanks to the NSF for funding this work. And I'd be happy to take any questions. (audience applauding) - [Moderator] Thank you. We have time for a couple questions. Over there, yes? Please remember to state your name. - [Dan] Hi, Dan Russel from Google. I'm wondering in your Mechanical Turk studies, to what extent you controlled for different conditions of the displays, because you don't know who you're getting really. They could have an ancient display. They could be running on a CRT for all you know. And so I've noticed when I've done my own studies like this, that there's huge variation between monitors, and I worry that that might affect some of these results, because for example, in some of the whiter colors we've seen, they just disappear if the monitor is not tilted at exactly correct angle. So how did you account for that kind of differences? - So that's a great point, and it is a trade-off for sure. It's a trade-off basically between control over viewing conditions. And I should say we didn't really apply strict control over viewing conditions through this study. But it's a trade-off of control over viewing conditions for more ecological validity, because our goal here is to not necessarily to learn the precise perceptual mechanisms at play with some of these interactions, but to really give visualization designers a more practical guide to use to improve their encodings. And so visualization designers also do not know the hardware setups of their users. And so it's definitely a trade-off and a limitation of the study, but we think that basically we recruited a very large number of participants to help mitigate that a little bit, and also we wanna just focus on the more practical guidelines to help real world designers. - [Audience Member] (mumbles) from the University of St Andrews. I was wondering, 'cause there's potentially a lot of combinations here, of visual variables that affect other variables. And just by showing the non-symmetry, you just multiply the whole thing by two. I was wondering whether you had any thoughts about how we can deal, not individually as researchers, but as a community to come up with models that are a little bit more comprehensive. How are we gonna build up this knowledge so that we get better, and it's not one individual study after another that we cannot combine? - Yeah, that's a great point. Basically, if we're able to get sort of an intuition as to the deeper causes of some of these interferences in perceptions, then we perhaps can run more control like vision-science-oriented experiments, kind of related to this previous question where we maybe strap people's heads in and get a real solid intuition of what's happening with the visual system. Then maybe we can run sort of a broader experiment that allows us to build more generalizable models, instead of, like you're saying, having to run specific experiment after specific experiment so I think, experiments like these might give us insight into how we can design future experiments to help address that. But I think it's a great point, yeah. - [Audience Member] Thank you. - [Moderator] And that's (coughs), excuse me, that's time. So let's thank Steve one more time and all of our presenters from this segment. (audience applauding)
