{"title": "tempTitle", "slideCnt": 22, "groundTruthOutline": [{"sectionTitle": "title", "startSlideIndex": 1, "endSlideIndex": 1}, {"sectionTitle": "1 INTRODUCTION", "startSlideIndex": 2, "endSlideIndex": 3}, {"sectionTitle": "3 DATA COLLECTION", "startSlideIndex": 4, "endSlideIndex": 5}, {"sectionTitle": "1 INTRODUCTION", "startSlideIndex": 6, "endSlideIndex": 7}, {"sectionTitle": "3 DATA COLLECTION", "startSlideIndex": 8, "endSlideIndex": 10}, {"sectionTitle": "4 ANALYSIS", "startSlideIndex": 11, "endSlideIndex": 14}, {"sectionTitle": "5 DISCUSSION", "startSlideIndex": 15, "endSlideIndex": 15}, {"sectionTitle": "6 CONCLUSION'", "startSlideIndex": 16, "endSlideIndex": 21}], "annotations": {"f84e9a56367a4a4ebfb2b69f8febc0e0": [{"sectionTitle": "Title", "startSlideIndex": 1, "endSlideIndex": 1}, {"sectionTitle": "Background and problem", "startSlideIndex": 2, "endSlideIndex": 6}, {"sectionTitle": "Two research questions", "startSlideIndex": 7, "endSlideIndex": 7}, {"sectionTitle": "Methodology", "startSlideIndex": 8, "endSlideIndex": 10}, {"sectionTitle": "Results", "startSlideIndex": 11, "endSlideIndex": 15}, {"sectionTitle": "Conclusion of this study", "startSlideIndex": 16, "endSlideIndex": 16}, {"sectionTitle": "Ongoing their campaign (application of this study)", "startSlideIndex": 17, "endSlideIndex": 20}, {"sectionTitle": "end", "startSlideIndex": 21, "endSlideIndex": 21}]}, "slideInfo": [{"index": 0, "startTime": 0, "endTime": 3.33, "script": "", "ocrResult": " text Hig CUMS) UO MMA Vee sens JSSIGCHI COPYRIGHT \u00a9 2019 ACM."}, {"index": 1, "startTime": 3.33, "endTime": 16.67, "script": "- Hello. Okay so yes, I'm Mark Cartwright from NYU, and I'll be talking about crowdsourcing multi-label audio annotation tasks with citizen scientists. So first of all context, so I'm a researcher", "ocrResult": " text wdsourcing Multi-label Audio Anno\u2019 ks with Citizen Sc Mark Cartwright, Graham Dove, Ana E Juan P. Bello, Oded Nov New York Unive Depa EE MARL sofive>)"}, {"index": 2, "startTime": 16.67, "endTime": 31.33, "script": "on the Sounds in New York City project, SONYC, which is a large project aimed at monitoring, analyzing, and mitigating urban noise pollution in New York City. Which has a lot of health and quality of life effects in the city.", "ocrResult": " text sofiro>)"}, {"index": 3, "startTime": 31.33, "endTime": 46.0, "script": "In particular I work in the machine listening crowdsourcing sides of the project and our goal is to build multi-label models that can detect multiple sound events in a scene to support our data-driven analysis and build tools for both citizens and city agencies.", "ocrResult": " text sofiro>))"}, {"index": 4, "startTime": 46.0, "endTime": 68.67, "script": "And we have deployed 60 sensors which have collected 130 million recordings that are 10 seconds long which equates to around 41 years worth of audio if you concatenate it all together. And this is the audio that we're gonna use to train our models but of course we need to label some of this audio. So in conjunction with the Department of", "ocrResult": " text sofiro>)) 130,000,000"}, {"index": 5, "startTime": 68.67, "endTime": 100.0, "script": "Environmental Protection in New York City and through inspecting the noise code we've developed a set of 23 fine level sound classes that are grouped into eight coarse level sound class categories for labeling our audio. And so this isn't really an exhaustive set of sounds but it's aimed to be descriptive and actionable while still being feasible for people to label and for us to train models on.", "ocrResult": " text"}, {"index": 6, "startTime": 100.0, "endTime": 139.33, "script": "So due to the public interest in this topic we've actually turned to citizen scientists to help us annotate some of this data. So we launched a campaign on Zooniverse which is the largest citizen science platform. But because of the temporal dimension of audio, audio annotation is much more time intensive than the image annotation tasks that really dominate Zooniverse. You can't just glance and click to annotate. So this had us question which annotation norms we should adopt on this task. And led us to the following research question", "ocrResult": " text"}, {"index": 7, "startTime": 139.33, "endTime": 239.33, "script": "which is how does the type of multi-label annotation task affect throughput and quality? Should we adopt the norms of paid crowdsourcing tasks, paid crowdsourcing audio tasks, rather, and break annotation into multiple binary annotation tasks, or should we adopt the norms of image annotation with citizen scientists like the tasks on Zooniverse and use multi-label annotation tasks? Therefore we started collecting our labels, started collecting labels of our sonic recordings on Zooniverse while simultaenously running a small study which aimed for high ecological validity. The study wasn't necessarily conducted as a controlled experiment but rather was an analysis of a real world data collection, you know solving real world audio annotation problems. We wanted authentic behaviors of volunteering citizen scientists which may differ from those of paid workers on commercial platforms. So to measure the quality of the labels we mixed in the occasional recording from YouTube for which we have some ground truth into our set of recordings for annotation. And we tested three task variations all of which gave us full multi-label annotations. We tested multiple paths of binary labeling, single pass multi-labeling, and somewhat hybrid multi-pass hierarchical multi-labeling. We had 339 participants and we asked five annotators per recording to annotate the multi-label tasks and for practical purposes we only collected three annotators per label for the binary but doing full multi-label annotation which I'll explain in a minute.", "ocrResult": " text joes the type of multi-label annotation tas 4 affect throughput and quality\u2019 + Do we adopt norms of paid crowdsourcing into multiple binary annotation ta: 1 do we adopt norms of 1\u00a2 annotation with c multi-label annotation ta"}, {"index": 8, "startTime": 239.33, "endTime": 264.0, "script": "So given this sound recording, (loud construction equipment) So for the binary labeling task in which it asks is there a jackhammer present in this recording let's say, I would say yes, there's a jackhammer present", "ocrResult": " text"}, {"index": 9, "startTime": 264.0, "endTime": 288.0, "script": "the high level classes such as impact tools", "ocrResult": " text \\nnotation Tas"}, {"index": 10, "startTime": 288.0, "endTime": 307.33, "script": "", "ocrResult": " text \\nnotation Tas"}, {"index": 11, "startTime": 307.33, "endTime": 340.67, "script": "So what do we find? So in regards to the annotation throughput we found that binary labeling generated more overall positive labels per recording. And the multi-label and hierarchical multi-label tasks generated about the same number of positive labels per recording. And as we'll see in a minute when there's more labels generated like in the binary labeling tasks this leads to higher recall in terms of the annotation quality which we'll see in a minute.", "ocrResult": " text Annotation Throughput + Binary labeling task generated more overall positive labels per recording YouTube Recordings Mutt-tobe! 1.25 Hrchl. Mutt-tabe! a oe 1 S31 ES + Mean number of generated labels per recording"}, {"index": 12, "startTime": 340.67, "endTime": 398.0, "script": "And also in regards to annotation throughput the binary labeling task, so the individual task, took half as long as the multi-label task for an individual annotation. Now a note about that is that we could do simply do positive and negative sampling of examples for binary annotation and not get the full multi-label annotation which you would have, let's say, we classifiers go through your data, find a, propose a list of recordings that it thinks are positive and then have those verified by annotators. However this requires good sampling mechanisms which is the case for some of the classes but many of the classes that is actually not the case. So in addition there's also been some studies that have shown that full multi-label annotations actually lead to better machine listening models as well.", "ocrResult": " text Annotation Throughput + Binary labeling task took hi ong \u00ab jabel for an individual annotation Time to Complete Individual Annotation Task Binary |} \u2014\u2014\u2014 jensen rch. Mi Hrehi. Me Time (s)"}, {"index": 13, "startTime": 398.0, "endTime": 420.0, "script": "doing binary annotation for all 23 classes of course that's gonna lead to a much longer time for getting complete, full multi-label annotation. And multi-label and hierarchical multi-label are much less in that sense.", "ocrResult": " text Annotation Throughput + However, for a full 23 class multi-label annotation binary labeling took 9x long as multi-labeling Time to Complete Full Multi-label Annotation =\u2014 = Multilabel_ Hrchl. Multi-tat Binary"}, {"index": 14, "startTime": 420.0, "endTime": 456.67, "script": "So in these plots here we have the f score on the top row, precision in the middle, and recall at the bottom, and multi-label annotation, then hierarchical and binary from left to right. And on the x-axis you have the minimum votes for a positive label. So let's say if only one annotator said that this class was present, it would get a positive label. And then the colors are the number of annotators that we asked to do the task. So here we find that volunteers tend to", "ocrResult": " text Annotation Quality"}, {"index": 15, "startTime": 456.67, "endTime": 566.67, "script": "So we were able to get some feedback from participants but due to the kind of constraints of the platform we weren't able to really survey all of the people so in the minimal feedback we were able to get we found that in general when given the binary labeling tasks the participants in the crowdsourcing platform felt underutilized. They wanted to do more but they couldn't and were frustrated. And this is a very different conclusion than people have come up with when doing similar tasks in paid crowdsourcing environments when given the multi-label task people actually in that task they actually become frustrated rather than the opposite. And they find it more overwhelming.", "ocrResult": " text Feedback from Participants (Binary Labelin + \u201cThere might be a better way than is that X sound yes or no to classify quicker. People will get tired of listening to sound clips faster than other quick options, like the animal diaries. You want to squeeze as much data out of each audio clip.\u201d 1 hear drums yelling applause, at least one large size dog that is very unhappy abs This takes place outside. | have no way to label more than two features, so it will probably be more frustrating than I can deal with to participate.\u201d In my opinion, this project should use the same model as the animal camera trap projects, that is, have a list of sound categories that one can click on for each clip, and give the opinion to choose more than one category.\u201d"}, {"index": 16, "startTime": 566.67, "endTime": 630.67, "script": "that overall quality of multi-label annotations from binary and multi-label tasks are pretty comparable, they have differences but they can be balanced through the way we aggregate the data. And multi-label annotation's much more efficient but only if you really need multi-label annotation. If you can get by with this kind of positive negative sampling, binary annotation can be efficient as well. In the hierarchical multi-label tends to propagate error leading to lower recall and in informal feedback that we received we indicated that volunteers much preferred the multi-label which the opposite of the paid crowdsource workers. And these results really side with the common practice of citizen science image annotation rather than that of paid audio crowdsourcing. So those sort of norms from the image annotation do go over to the audio annotation side as well.", "ocrResult": " text \u00bbnclusions of Study Overall quality of multi-label annotations from binary and multi-label tasks comparable. They have differences but they can be balanced. Multi-labet i h more efficient, but only if you need full multi-label annotation Hierarchical multi-label tends to propagates error, leading to lower recall Informal feedback indicates that volun\u2019 much preferred multi-label, opposite of paid crowdworkers Results side with the common practice of ence image annotation rather than that of paid audio crowdsourcing."}, {"index": 17, "startTime": 630.67, "endTime": 670.0, "script": "So we've been continuing with this since we need to collect our data so we have an ongoing citizen science annotation campaign and thus far we've collected data from about 1,000 annotators, registered annotators rather, there's actually a couple thousand more unregistered annotators, and have received around 30,000 full multi-label annotations, audio annotations, and for around 10,000 completed audio recordings in which we got three annotators per recording. And our goal is to get about 1,000 positive examples", "ocrResult": " text 1,051 30,376 jegistered Full Multi-labet Annotator Annotatior"}, {"index": 18, "startTime": 670.0, "endTime": 695.33, "script": "the way data is distributed in actual natural recordings is very skewed towards these common classes so we're still fairly far from our goal. The green here is when we've gotten three annotators per label, they're the green in that label, and orange in two, blue one. So we still have a bit more work to do.", "ocrResult": " text Annotation Campaign"}, {"index": 19, "startTime": 695.33, "endTime": 726.67, "script": "We've actually released some of this data as part of a challenge which is part of the Detection and Classification of Audio, sorry, Acoustic Scenes and Events which we're hosting at NYU in October. So we released about close to 3,000 recordings. Some of these are training recordings and over 400 of them we annotated ourselves for validation and these have three Zooniverse annotators per recording.", "ocrResult": " text E 2019 Challenges Tz oustic scene classification \u00a9 audio tagging with noisy labels and minimal supervision und event localization and detection sound event detection in domestic environment Sound Tagging"}, {"index": 20, "startTime": 726.67, "endTime": 752.67, "script": "and compare it to how the crowdsourced annotators are comparing to the sonic research team they're doing pretty well on the coarse level but in the fine level there's still a lot of confusions. There's a lot more work that needs to be done to improve the quality of those. So we're looking into incorporating more experts into our loop to improve the quality of these fine level annotations.", "ocrResult": " text SONYC Urban Sound Tagging Dataset oniverse volunteers compare to those of the"}, {"index": 21, "startTime": 752.67, "endTime": 1014.0, "script": "And that's it. Any questions? (audience applauds) - [Jenny] Thank you, Jenny Priess, thank you for your presentation. I understand that this is a sort of methodological experiment but can you give us some ideas of when it might be useful to be able to separate out different sounds. I mean the one thing I can think of is bird song in woodlands but a lot of the time I think people want to know the decibel level of the sound. I live opposite to a place where building is going on and I actually don't care too much about what's making the sound, whether it's jackhammers or whatever else it is, but what I do care a lot about is the level of the sound that wakes me up early in the morning. - Sure. - [Jenny] The decibel level. - Sure yeah, so our sensors are recording decibel level, but in order to actually kind of make the decibel level actionable we need to know what kind of sound it is, right? So in order for the, there's different, for instance the Department of Environmental Protection, they have different, in the noise code, there's different levels for each of the different types of sounds of what is tolerable in the city, what the city has at least defined as tolerable, right? So if they know that a pile driver for instance, which is you know, driving big piles down into the ground, laying the foundation for something, if that's exceeding the noise level at a certain time period that's a really big deal. Whereas if it's, you're complaining, about your neighbor's dog or something like that that's making a bunch of noise, you know, that's kind of, the city treats that differently. - [Jenny] But that's not normally sorting out a whole set of different sounds, is it, as you're doing in your experiment? - So we need to be able to identify those sounds from our sensors to make that information actionable for city agencies. - [Jenny] Yeah, okay. I guess what I really question is the realism of the results that you are getting in real term applications, but anyway, thank you. - Okay, so connecting the loop, I guess. So, oh, okay, whatever, but. (laughter) - [Jenny] Thank you. - [Sandy] Hello, I'm Sandy Gould. Do you have any insight into the relationship between so you're using citizen scientists to do this, do you have any insight into sort of, the results are nice and clear and lay out really well, do you have any insight into how these people are taking on these tasks and how they're working on them and how that relates to it? So obviously the binary one doesn't perform all that well but they're quick and easy for people to do, right? They're not very taxing relatively speaking. I thought there was an interesting comment there about trying to extract more information from it. Do you get much insight from the way people are working on these tasks and how that might influence the choice of which kind of selection approach you take? Does that make any sense? - Um... - [Sandy] No? (laughter) - Can you be a bit more specific? - [Sandy] So you've got three different ways of people completing this task. - Yes. - [Sandy] And they perform in their own terms at the different levels but how do they fit with how people's experience of completing these tasks, right, the actual people doing it at the other end of the wire? - Sure, yeah, so I mean, as I said, due to constraints of the platform we weren't really, we originally had like a really big complete survey on there, we had all this other stuff, and they actually had us remove all of that before we launched the task 'cause they really try to make the tasks really easy for people to complete and you don't want to ask too much of people in terms of oh you need to follow this link and do this survey and whatnot, so. It's not, Zooniverse really isn't a platform for experimentation that much, right, so we weren't really able to collect that data. So we had some feedback from participants through like talkboards and direct messages to us and we tried to message a bunch of people to try to get more information from them, but it just really wasn't possible. So we unfortunately don't know much about some of those questions at the moment, just except for this little bits of informal feedback that we had. - [Sandy] Yeah, thanks very much, it's not an issue with the results at all, just I was just curious, thank you, cheers.", "ocrResult": " text Sonclusions of Study Overall quality of multi-label annotations from binary and multi-label ta comparable. They have differences but they can be balanced. Multi-label is much more efficient, but only if you need full multi-label annotation Hierarchical multi-label tends to propagates error, leading to lower recall Informal feedback indicates that volunt preferred multi-label, opposite of paid crowdworkers Results side with the common practice of ence image annotation rather than that of paid audio crowdsourcing."}], "topSections": [[["1 INTRODUCTION", 0.0], ["2 RELATED WORK", 0.0], ["3 DATA COLLECTION Platform", 0.0], ["4 ANALYSIS", 0.0], ["5 DISCUSSION", 0.0], ["6 CONCLUSION", 0.0]], [["3 DATA COLLECTION Platform", 0.06], ["6 CONCLUSION", 0.06], ["1 INTRODUCTION", 0.12], ["2 RELATED WORK", 0.14], ["4 ANALYSIS", 0.2], ["5 DISCUSSION", 0.42]], [["6 CONCLUSION", 0.01], ["3 DATA COLLECTION Platform", 0.09], ["2 RELATED WORK", 0.11], ["4 ANALYSIS", 0.18], ["1 INTRODUCTION", 0.19], ["5 DISCUSSION", 0.42]], [["6 CONCLUSION", 0.04], ["3 DATA COLLECTION Platform", 0.08], ["2 RELATED WORK", 0.13], ["1 INTRODUCTION", 0.14], ["4 ANALYSIS", 0.19], ["5 DISCUSSION", 0.42]], [["6 CONCLUSION", 0.0], ["1 INTRODUCTION", 0.03], ["2 RELATED WORK", 0.11], ["4 ANALYSIS", 0.18], ["3 DATA COLLECTION Platform", 0.19], ["5 DISCUSSION", 0.49]], [["6 CONCLUSION", 0.02], ["1 INTRODUCTION", 0.11], ["4 ANALYSIS", 0.11], ["2 RELATED WORK", 0.14], ["3 DATA COLLECTION Platform", 0.14], ["5 DISCUSSION", 0.48]], [["6 CONCLUSION", 0.03], ["3 DATA COLLECTION Platform", 0.08], ["2 RELATED WORK", 0.16], ["1 INTRODUCTION", 0.18], ["4 ANALYSIS", 0.18], ["5 DISCUSSION", 0.37]], [["6 CONCLUSION", 0.03], ["2 RELATED WORK", 0.13], ["3 DATA COLLECTION Platform", 0.14], ["1 INTRODUCTION", 0.17], ["5 DISCUSSION", 0.22], ["4 ANALYSIS", 0.31]], [["2 RELATED WORK", 0.03], ["6 CONCLUSION", 0.04], ["1 INTRODUCTION", 0.06], ["3 DATA COLLECTION Platform", 0.14], ["4 ANALYSIS", 0.26], ["5 DISCUSSION", 0.47]], [["1 INTRODUCTION", 0.01], ["6 CONCLUSION", 0.06], ["2 RELATED WORK", 0.1], ["3 DATA COLLECTION Platform", 0.19], ["5 DISCUSSION", 0.26], ["4 ANALYSIS", 0.38]], [["1 INTRODUCTION", 0.0], ["2 RELATED WORK", 0.0], ["5 DISCUSSION", 0.09], ["6 CONCLUSION", 0.18], ["3 DATA COLLECTION Platform", 0.22], ["4 ANALYSIS", 0.51]], [["6 CONCLUSION", 0.01], ["2 RELATED WORK", 0.04], ["1 INTRODUCTION", 0.06], ["3 DATA COLLECTION Platform", 0.11], ["5 DISCUSSION", 0.36], ["4 ANALYSIS", 0.42]], [["6 CONCLUSION", 0.01], ["2 RELATED WORK", 0.07], ["1 INTRODUCTION", 0.11], ["3 DATA COLLECTION Platform", 0.11], ["4 ANALYSIS", 0.35], ["5 DISCUSSION", 0.35]], [["6 CONCLUSION", 0.02], ["1 INTRODUCTION", 0.11], ["3 DATA COLLECTION Platform", 0.11], ["2 RELATED WORK", 0.17], ["5 DISCUSSION", 0.26], ["4 ANALYSIS", 0.33]], [["6 CONCLUSION", 0.01], ["3 DATA COLLECTION Platform", 0.07], ["1 INTRODUCTION", 0.1], ["2 RELATED WORK", 0.11], ["4 ANALYSIS", 0.35], ["5 DISCUSSION", 0.36]], [["6 CONCLUSION", 0.0], ["2 RELATED WORK", 0.02], ["1 INTRODUCTION", 0.05], ["4 ANALYSIS", 0.09], ["3 DATA COLLECTION Platform", 0.1], ["5 DISCUSSION", 0.74]], [["3 DATA COLLECTION Platform", 0.05], ["6 CONCLUSION", 0.05], ["2 RELATED WORK", 0.14], ["1 INTRODUCTION", 0.15], ["4 ANALYSIS", 0.27], ["5 DISCUSSION", 0.34]], [["6 CONCLUSION", 0.02], ["1 INTRODUCTION", 0.1], ["3 DATA COLLECTION Platform", 0.11], ["2 RELATED WORK", 0.16], ["5 DISCUSSION", 0.27], ["4 ANALYSIS", 0.34]], [["6 CONCLUSION", 0.01], ["2 RELATED WORK", 0.08], ["1 INTRODUCTION", 0.11], ["3 DATA COLLECTION Platform", 0.12], ["4 ANALYSIS", 0.3], ["5 DISCUSSION", 0.38]], [["6 CONCLUSION", 0.0], ["1 INTRODUCTION", 0.03], ["2 RELATED WORK", 0.08], ["3 DATA COLLECTION Platform", 0.13], ["4 ANALYSIS", 0.29], ["5 DISCUSSION", 0.47]], [["6 CONCLUSION", 0.04], ["2 RELATED WORK", 0.07], ["3 DATA COLLECTION Platform", 0.08], ["1 INTRODUCTION", 0.14], ["4 ANALYSIS", 0.22], ["5 DISCUSSION", 0.45]], [["1 INTRODUCTION", 0.0], ["2 RELATED WORK", 0.0], ["3 DATA COLLECTION Platform", 0.0], ["4 ANALYSIS", 0.0], ["5 DISCUSSION", 0.0], ["6 CONCLUSION", 0.0]]], "outline": [{"sectionTitle": "title", "startSlideIndex": 1, "endSlideIndex": 1}, {"sectionTitle": "4 ANALYSIS", "startSlideIndex": 2, "endSlideIndex": 2}, {"sectionTitle": "2 RELATED WORK", "startSlideIndex": 3, "endSlideIndex": 3}, {"sectionTitle": "5 DISCUSSION", "startSlideIndex": 4, "endSlideIndex": 20}, {"sectionTitle": "end", "startSlideIndex": 21, "endSlideIndex": 21}], "weights": [-1, 7.2, 6.97, 6.669999999999998, 6.47, 6.26, -1], "similarityTable": [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12, 0.14, 0.06, 0.2, 0.42, 0.06], [0.19, 0.11, 0.09, 0.18, 0.42, 0.01], [0.14, 0.13, 0.08, 0.19, 0.42, 0.04], [0.03, 0.11, 0.19, 0.18, 0.49, 0.0], [0.11, 0.14, 0.14, 0.11, 0.48, 0.02], [0.18, 0.16, 0.08, 0.18, 0.37, 0.03], [0.17, 0.13, 0.14, 0.31, 0.22, 0.03], [0.06, 0.03, 0.14, 0.26, 0.47, 0.04], [0.01, 0.1, 0.19, 0.38, 0.26, 0.06], [0.0, 0.0, 0.22, 0.51, 0.09, 0.18], [0.06, 0.04, 0.11, 0.42, 0.36, 0.01], [0.11, 0.07, 0.11, 0.35, 0.35, 0.01], [0.11, 0.17, 0.11, 0.33, 0.26, 0.02], [0.1, 0.11, 0.07, 0.35, 0.36, 0.01], [0.05, 0.02, 0.1, 0.09, 0.74, 0.0], [0.15, 0.14, 0.05, 0.27, 0.34, 0.05], [0.1, 0.16, 0.11, 0.34, 0.27, 0.02], [0.11, 0.08, 0.12, 0.3, 0.38, 0.01], [0.03, 0.08, 0.13, 0.29, 0.47, 0.0], [0.14, 0.07, 0.08, 0.22, 0.45, 0.04], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], "scriptSentences": [[" Annotating rich audio data is an essential aspect of training and evaluating machine listening models, which have the potential to enable powerful applications in diverse domains such as bioacoustic monitoring, urban noise monitoring, electric vehicle sensing, assistive technologies, and more.", " This is a difcult and time consuming task, due in part to audio\u2019s temporal dimension.", " We approach this problem from the context of a New York City-based project that is working closely with city agencies such as the Department of Environmental Protection; and which aims to monitor, analyze, and mitigate urban noise pollution using a smart sensor network powered by machine listening models that detect noise sources [5].", " Noise pollution is a major concern to many urban residents and has many negative efects, e.g.", " on citizens\u2019 health [4, 21] and students\u2019 learning [4].", " Because of these societal concerns, we are exploring citizen science-based inquiry, including our approach to training machine listening models, and a key task is to establish how best to design tasks to acquire multilabel audio annotations with volunteers.", " Since our aim is for high ecological validity, this study was not conducted as a controlled experiment, but rather we undertake an analysis of a real-world data collection solving a real-world audio annotation problem.", " This activity took place on Zooniverse, the most widely used citizen science platform, and as such,", " our focus is on the authentic behaviors of volunteering citizen scientists, which may difer from those of paid workers on commercial platforms.", " Best practice for crowdsourced audio annotation remains understudied, with prior research typically focusing on paid crowdworkers rather than volunteer citizen scientists.", " In this paper we make some initial steps towards addressing this area of concern.", " Previous research recommends breaking complex tasks into small units of work for paid crowdsourcing [31].", " In large-scale audio annotation eforts, this has resulted in paid crowdworkers performing single-class binary-labeling tasks [20, 22].", " But citizen scientists and crowdworkers have diferent motivations, and in online citizen science projects, where audio annotation is rare, image annotation typically adopts a multi-labeling approach [34, 51].", " However, like video annotation, annotating audio is more complex than annotating images due to the addition of a temporal dimension; and while full multi-label annotations for N classes can be constructed from N binary class annotations, this does not scale well.", " Should researchers follow the norms of citizen science image annotation (multi-label) when collecting multi-label audio annotations with volunteers?", " Or those of crowdsourced audio annotation with paid crowdworkers (multiple binarylabel)?", " What are the efects on annotation throughput and quality of adopting these contrasting approaches?", " Two previous studies point in contrasting directions.", " In [22], a pilot study indicated that crowdworkers found a multi-label audio annotation task difcult, were unhappy, and had low annotator agreement.", " However, in [50], a study on video annotation with crowdworkers found that multi-label annotation tasks resulted in higher quality annotations than those from binary annotation tasks.", " This paper contributes to the literature on best practices for crowdsourced audio annotation, and seeks to answer these questions, by comparing diferent annotation task types on the Zooniverse online citizen science platform [1].", " We compare multiple-pass binary annotation, single-pass multilabel annotation, and a hybrid approach: hierarchical multipass multi-label annotation; in work from the early stages of our urban soundscape annotation campaign.", " We present an analysis of the throughput of the three diferent annotation task types and a sensitivity analysis on the efect of aggregation variables on annotation quality.", " Our results suggest that practices developed for crowdsourced populations may not translate to volunteer-based populations."], [" In the past decade, there have been several large-scale, multilabel and multi-class paid-crowdsourcing eforts to annotate media for training machine learning models, e.g.", " ImageNet (14M images, 20k classes) [15], COCO (328k images, 91 classes)[33], Places (10M images, 434 classes) [56], AudioSet", " (1.8M audio recordings, 636 classes) [20], and OpenMIC-2018 (20k audio recordings, 20 classes) [22].", " While each has taken a slightly diferent approach to suit their media, they all use weak search engines [15, 20, 33, 56] or classifers [22] to generate candidate sets which are then verifed by humans with binary [15, 20, 22, 56], limited multi-label [20], or hierarchical multi-label [33] annotation.", " Of these eforts, AudioSet [20] and OpenMIC-2018 [22] are the only audio annotation eforts.", " It is also worth noting that AudioSet annotators were exposed to both audio and accompanying video during annotation because they found it too difcult with audio alone.", " COCO [33] is the only dataset with full multi-label annotations, and required 70k worker hours to complete image segmentation and labeling.", " Because of the immense human efort required, researchers have sought to optimize multi-class and multi-label crowdsourced image annotation [7, 12, 13, 16, 32], often with approaches that break larger multi-label tasks into small subtasks.", " In audio annotation, studies have typically focused on the efects of sound visualizations rather than trade-ofs between binary and multi-label annotation when investigating rapid methods [53] and best practices [10].", " However, a small-scale study using data labeled by experts rather than through crowdsourcing indicates that sound event detection accuracy is higher when using a single model trained on full multi-label data rather than aggregating multiple single-class models trained on binary data [8].", " For video annotation, which like audio has the additional complexity of a temporal dimension, research has indicated both that breaking annotation into smaller subtasks is wasteful [50], and also conversely that performance and annotator satisfaction increase when annotating one object rather than multiple objects, (although error accumulated in sequences of micro-tasks largely negated this fnding) [54].", " All of the above annotation eforts and studies were performed with paid crowdworkers.", " While intrinsic motivations can afect the quantity [11] and quality [46] of paid crowdwork, the primary motivation of paid crowdworkers is still payment and therefore extrinsic [30].", " It\u2019s therefore unclear if the fndings of studies using paid crowdsourced annotation will translate to annotation with citizen scientists, who are not motivated by payment.", " Viewing the problem from the opposite direction, [36] show that while paid crowdworkers can complete labeling tasks with comparable quality to volunteer citizen scientists, they are also highly sensitive to changes in payment method.", " Zooniverse [1], the largest online citizen science platform, has hosted several large image annotation projects, most notably Galaxy Zoo (900k images, 6 classes) [34] and Snapshot Serengeti (1.2M image sets, 48 animal presence classes) [51], but audio annotation projects on the platform are rarer and smaller in size [35, 48].", " On the Zooniverse platform,", " image-based projects typically use full multi-label annotation.", " Although the initial Galaxy Zoo project was limited to six classes, there are now several projects inspired by Snapshot Serengeti that have on the order of 50 classes [1].", " Unlike paid crowdsourcing, citizen science relies on voluntary eforts of contributors.", " Volunteers typically express multiple motivations [42\u201344], which change dynamically through a project\u2019s temporal shifts [47].", " Typically, online citizen science projects have skewed participation patterns, with a small number of highly motivated volunteers contributing the majority of work [19].", " The quantity of online volunteer contributions correlates with collective-, norm-oriented-, reputation-, and intrinsic- motivation; while contribution quality responds positively to collective- and reputationmotives [40].", " Volunteers\u2019 individual contributions may be increased through highlighting instances of novelty [25].", " The importance of intrinsic motivations highlights a need for engaging participation mechanisms, and association with a project\u2019s aims helps motivate new volunteers [39].", " Where volunteers are drawn from a particular community, motivations for engagement in citizen science closely refect motivations for community membership of that community and its core activities [55].", " The desire to learn about a particular topic is an important motivator in both online [42, 43] and in-real-life [18] citizen science, as is the desire to contribute to \u201cauthentic\u201d scientifc research [38].", " In addition to knowledge acquisition, sharing new knowledge can motivate and sustain engagement [26].", " Other methods of motivating sustained engagement include competition amongst volunteers and gamifcation of citizen science activities [6, 24, 41].", " Such regular engagement with a project builds community membership, and leads to task aptitude and familiarity; while task difculty and boredom, and competing priorities are barriers to contribution [28].", " Other factors that impact on volunteers\u2019 engagement, both positively and negatively, include a project\u2019s coordination practices and the volunteer\u2019s previous domain experience [17].", " In addition, scientists\u2019 concerns about data quality and interpretation, and wider ethical concerns about visibility, authorship and attribution [14, 45], can lead to tension between the motivations and epistemic contributions of citizen science participants as individuals and their status within a collective distributed efort [29].", " We build on this literature by investigating best practices for crowdsourced audio annotation with volunteer citizen scientists."], [" Zooniverse is the largest platform for online citizen science projects [1], ofering a ready-made community of motivated volunteers from which to recruit.", " While this facilitates our", " aim of studying the authentic behaviors of volunteering citizen scientists, the Zooniverse platform is designed for real-world data collection and not for controlled experiments, and researchers therefore have limited facility for controlling variations over the presentation of their tasks.", " Also, during the design stage of this research, we were in close contact with expert Zooniverse moderators who provided informed advice, suggesting practical compromises between ensuring that primary data were collected and enriching these primary annotation data with user-focused data.", " For example, this expert advice included removing links to external questionnaires in order to \u201clower the barrier to entry as much as possible\u201d.", " However as an alternative, the Zooniverse platform does enable researchers to view more qualitative participant responses through its \u2018Talk\u2019 boards.", " Suggestions such as these highlight tensions between eliciting real-world annotation with citizen science volunteers and gathering rich participant data.", " Because of factors such as this, our comparison of multiple annotation task types in the early stages of our data collection should be considered more akin to A/B testing than strictly controlled experimentation.", " Our audio data consist of 10 second clips from two distinct sources, and represent 22 classes of sound sources.", " The frst dataset consists of audio taken from selected YouTube video clips, where the sound source was identifed, and additionally confrmed by members of our research team.", " These provided ground truth for our analysis.", " The second were recordings selected from amongst the 30 years\u2019 worth of audio data collected by 50+ sensors installed in busy locations around New York City [5].", " The 22 classes of sound sources that make up our labeling taxonomy are derived from requirements requested by the city\u2019s department of environmental protection, and based on its legally enforceable noise code.", " They include examples of engines of diferent sizes, construction machinery and tools, human and animal vocalizations, music, and vehicle alert signals and sirens.", " We selected 2 examples for each class from the YouTube dataset, and 3 examples from the dataset of sensor recordings, creating a total of 110 10 second audio clips.", " The 3 examples selected from the sensor data were curated by members of our research team from an initial sample of 30 examples automatically selected for each class using \u201cVGGish\u201d embedding-derived audio features [27].", " Volunteers were randomly presented with 1 of 3 task types: 1) binary-labeling, 2) one-stage multi-labeling, and 3) two-stage hierarchical multi-labeling.", " All volunteers were presented with the same tutorial and feld guide to familiarize themselves with examples of each sound-source class.", " In all three", " task types, the audio was presented both aurally and visually (using a spectrogram representation; see Figure 1a).", " In the binary labeling task (see Figure 1b), volunteers were asked to decide whether a single suggested sound-source class was present or not in the recording.", " This task type provided both positive and negative labels explicitly.", " In the one-stage multi-labeling task (see Figure 1c), volunteers were presented with a list of 30 class labels and an audio clip, and were asked to select all the sound-source classes present in the audio.", " The list of label options included our 22 sound-source classes plus labels for unknown or uncertain examples of the \u201csuperclasses\u201d; e.g.", " engines, construction machinery, or alert signals.", " This task type provided positive labels explicitly and negatively labels implicitly.", " Previous studies [50] indicate that requesting explicit negative labels reduces both precision and recall, and increases task completion time in a multi-label task.", " In the two-stage hierarchical multi-label task, each stage was undertaken separately and by a diferent volunteer.", " In stage 1, the audio was presented to a volunteer alongside a list of 9 superclass labels; e.g.", " engines, or powered sawing", " tools.", " Identifcation of sounds in this stage provided a flter for possible class labels.", " For each selected superclass in stage 1, we posted a stage 2 task in which the same audio clip was presented alongside the sublist of our 22 class labels that correspond to the superclass.", " For example, if the audio had been identifed as containing engine sounds in stage 1, the list of possible labels shown in stage 2 would include: largesounding engine, medium-sounding engine, small-sounding engine, other/unknown engine, artifcial interference noise, and other/unknown sound.", " Stage 2 tasks were undertaken at a later date, and the audio was presented to a diferent volunteer.", " This task type provided positive labels explicitly and negative labels implicitly.", " We included this task as a compromise between the multi-labeling and binary labeling tasks \u2014 it requires fewer sound-source classes to be annotated simultaneously and possibly fewer sound-source classes to be annotated overall for full multi-label annotation."], [" We collected at least 5 annotations per recording for the multi-label task and both stages of the hierarchical multilabel task and at least 3 annotations per recording for the binary annotation task, from a total of 339 unique volunteers.", " We limited our analysis to these minimums.", " It was impractical to collect more than 3 annotations for the binary task, due to it\u2019s poor scalability.", " The annotator agreement for all three task types is presented in Table 1.", " We also downloaded the comments and messages that volunteer citizen scientists left on the Zooniverse \u2018Talk\u2019 boards as a way of approaching user-focused data.", " Because there were too few of these messages to undertake a detailed qualitative analysis, we have instead included example comments in our Discussion, as a way to illustrate particular points regarding Zooniverse volunteers\u2019 responses to the diferent annotation approaches, and not as study fndings in their own right.", " We defne the throughput of an annotation task as the rate of label generation.", " This is a function of the number of volunteers, the quantity of labels that they generate in a single task, the speed at which they complete a task, and the number of annotation tasks they are motivated to complete.", " Maximizing this measure helps collect data quickly and progress research.", " It also respects the time volunteer annotators freely give to the project as productive contributors.", " Our analysis focuses on the quantity of positive labels generated and the speed of task completion in response to the annotation task.", " To calculate task completion times, we computed the time diference between annotation tasks submitted by the same volunteer and removed outliers in the top 5th percentile, which may represent diferent annotation sessions.", " The binary annotation task generated over twice as many positive labels per full multi-label annotation as the multilabel annotation tasks, (see Figure 2).", " In addition, as shown in Figure 3, it took about twice as long to complete an individual multi-label annotation task (32.81 s, 95% CI [30.80, 34.86]) as it did an individual binary annotation task (14.06 s, 95% CI [13.74, 14.38]).", " When scaled up for 22 classes, it took more than 9 times as long to annotate a full multi-label annotation using binary labeling tasks (see Table 2).", " Therefore, if full annotations are needed, multi-labeling tasks have higher throughput; but if only binary annotations are needed, the two task types have comparable throughput.", " In addition to maximizing throughput, we also want high quality labels.", " To measure quality, we calculated the precision, recall, and F-measure on aggregated annotations of the YouTube recordings, since these have positive ground-truth labels.", " However, we also need negative ground-truth labels", " Hrchl.", " Multi-label S1 Hrchl.", " Multi-label S2 - Engine Hrchl.", " Multi-label S2 - Impact", " Hrchl.", " Multi-label S2 - Saw Hrchl.", " Multi-label S2 - Alert", " Hrchl.", " Multi-label S2 - Vocal Hrchl.", " Multi-label S2 - Music", " Figure 3: The time to complete individual annotation task for each annotation task type.", " S1 and S2 indicate stages 1 and 2 respectively.", " to compute our metrics.", " These we obtained by labeling a limited amount of data that we had high confdence about ourselves.", " Our ground-truth contained one positive and one negative label for each of the 44 YouTube recording and was balanced to have an equal number of positives and negatives for each class.", " With this ground-truth data, we varied both the number of annotators and the voting threshold required for a positive label, and measured their efects on annotation quality.", " To account for the many possible combinations of annotators, we estimated the true positives, false positives, and false negatives using a sample of 1000 random combinations of annotators for each task type / recording pair.", " For example, in one sample of the multi-labeling task aggregated with three annotators and a voting threshold of two, we randomly chose three of the fve annotations and labeled a class as positive if at least two of the annotators labeled it positive.", " We then calculated the true positives, false positives, and false negatives using our ground truth data, and repeated the process 1000 times.", " For hierarchical multi-labeling, we performed a similar process but aggregated annotations at both stages, with the output of stage one informing the inclusion of subtasks in stage two.", " The annotations from the subtasks were combined together to form a full multi-label annotation.", " For binary labeling, we aggregated binary annotations for each class and then combined all 22 class annotations together to form a full multi-label annotation.", " We then summed the true positives, false positives, and false negatives over the recordings to compute the quality metrics for each combination of", " task type, number of annotators, and voting threshold.", " Figure 4 shows the results of varying the number of annotators and the minimum voting threshold for all three task types.", " Using a three-way ANOVA, we investigated the efect of annotation task type, number of annotators, and voting threshold on the number of type I (false positive) and type II (false negative) errors for aggregate annotations.", " We computed the ANOVA directly on the type I and II errors rather than macro-averaged metrics because our limited ground-truth labels for each example and/or class could lead to ill-defned precision and recall metrics with macroaveraging.", " For the number of type I errors, we found that task type (F (2, 1548) = 81.62, p < 0.001) and voting threshold (F (4, 1548) = 7.15, p < 0.001) had signifcant efects, but number of annotators (F (4, 1548) = 1.59, p = 0.17) did not.", " There were also signifcant interactions between task", " type and voting threshold (F (6, 1548) = 13.75, p < 0.001) and task type and the number of annotators (F (6, 1548) = 3.54, p < 0.01).", " For the number of type II errors, we found that task type (F (2, 1548) = 129.74, p < 0.001), number of annotators (F (4, 1548) = 41.94, p < 0.001), and voting threshold (F (4, 1548) = 103.52, p < 0.001) all had signifcant efects.", " There were also signifcant interactions between voting threshold and the number of annotators (F (6, 1548) = 6.76, p < 0.001).", " To test where these diferences occurred, we also ran post hoc Tukey HSD tests (\u03b1 = 0.05), but we only report diferences in main efects for simplicity.", " For type I errors, we found signifcant diferences between the binary task and the two multi-label tasks but not between the two multilabel tasks themselves.", " In addition, we only found signifcant diferences in voting thresholds pairs 1\u20132 and 2\u20133.", " For type", " II errors, we found signifcant diferences between all task types and all thresholds, but did not fnd signifcant diferences in number of annotator pairs 1\u20132, 2\u20133, and 3\u20134.", " Overall, we found that with low voting thresholds, the binary task produced aggregate annotations with more type I errors (lower precision), and with high voting thresholds, the multi-label tasks produced aggregate annotations with more type II errors (lower recall).", " We didn\u2019t fnd any signifcant diferences between the two multi-label tasks in recall, but we found that the hierarchical multi-label task produced annotations with lower recall than the multi-label task as the voting threshold increased.", " At low voting thresholds for all tasks, we found minimal diferences in performance when the number of annotators was 3 and above, but more annotators was always better.", " The aggregate annotations from the multi-labeling and hierarchical multi-labeling tasks both attained their highest F-measures with fve annotators and a voting threshold of one annotator (0.96 and 0.92 respectively).", " Whereas, aggregate annotations from binary labeling achieved their highest F-measure with three annotators and a voting threshold of one annotator (0.94).", " For fair comparison, the max F-measure of multi-label annotations aggregated with three annotators was 0.93."], [" Multi-label audio annotation is a time consuming task for which few studies have investigated best practices.", " Our analysis suggests using a multi-label annotation task, collecting at least three annotations per example, and aggregating them with a low voting threshold will deliver results equal in quality to those collected using binary labeling, and will do so more quickly.", " We found annotators tended to \u201cover annotate\u201d when attending to one class at a time and to \u201cunder annotate\u201d when asked to attend to many classes.", " In light of these tradeofs, binary audio annotation may be preferred when high recall is prioritized, for example when training a gun shot detection model for which the cost of a false negative may be someone\u2019s life.", " And multi-label audio annotation may be preferred when precision is prioritized, for example when training an urban noise pollution detection model for which the cost of a false positive is an unnecessary investigation by a city noise inspector.", " When aggregated however, these tendencies can be balanced by adjusting the number of annotators and the voting threshold for each task type, making peak performance comparable.", " Also, the throughput of multilabel annotation was higher, which is advantageous when training a single model with multi-label output rather than multiple binary-class models.", " Such a single model may have higher accuracy [8], and we suspect that this diference may be greater when the model must distinguish between several similar classes.", " With only 30 classes, we did not fnd it advantageous to use the two-stage hierarchical multi-label model.", " At low voting thresholds, the hierarchical multi-label task produced labels of similar quality to the single-pass multi-label task, but at high voting thresholds, the downward trend on recall is more extreme than in the multi-label case, due to compounding type II errors at each annotation stage.", " While we suspect that an advantage for the hierarchical approach may appear as the number of classes is increased, additional experimentation is required to investigate this.", " We found the unanimous vote measure, 91% for multilabeling and 81% for binary labeling tasks, to be higher that the equivalent reported by the AudioSet data collection (76.2%), indicating greater consistency between citizen scientists than paid crowdworkers.", " However, we observed low scores when calculating Krippendorf \u03b1 agreement, indicating both that annotation of urban sound is a difcult task, and that there is room to improve the design of our tasks.", " Because we are presenting a study of data collection with an existing community of volunteer citizen scientists on the most well-established platform for these activities, and because we are using an intentionally restricted audio dataset, the following limitations should be acknowledged with respect to our analysis and fndings.", " Using the Zooniverse platform.", " Zooniverse is a platform designed for real-world citizen science data collection, rather than for controlled experimentation, and because of this it provides researchers with only limited control over the variations with which tasks and recordings can be presented.", " Therefore this was not a controlled study, rather it was more akin to an A/B test of diferent designs and measuring their impact.", " However, it is also important to recognize that fndings from this study should be considered subject to the potentially biasing impact of the particular norms associated with participation in Zooniverse projects.", " While it appears to us that our fndings are at least in part a refection of volunteer citizens scientists greater intrinsic motivation when compared to paid crowdworkers, it is also possible that they refect particular norms, standards, and expectations cultivated through volunteer participation in multiple Zooniverse projects over the years.", " As the refections of Zooniverse\u2019s UX team indicate [52], norms are emerging around the practices associated with large-scale citizen science participation on the platform, and while it is currently and quite signifcantly the largest instance of such a platform, Zooniverse is not the only option available.", " Future studies might compare the power of these norms by running simultaneous studies on diferent platforms.", " For example, do the fndings of previous studies, which both note the difering motivations", " of volunteer citizens sciences, e.g.", " [42\u201344] and also used data from Zooniverse volunteers, hold true for alternative platforms?", " Another possibility is that the kind of network afects seen with Google, Amazon and Facebook may apply in a citizen science context to Zooniverse.", " Either way, the choice of platform for similar studies in the future will remain an important consideration.", " These norms and practices also impacted our collection of user-focused data, as we followed the advice of expert moderators and chose to remove links to external questionnaires so as not to raise barriers to volunteers\u2019 participation.", " Instead we hoped that data downloaded from the comments and messages that volunteer citizen scientists left on the Zooniverse \u2018Talk\u2019 boards would be sufcient to provide similar insight.", " As it turned out, there were too few of these messages to undertake a detailed qualitative analysis, and so these merely provide illustration rather than fndings in their own right.", " Data from additional questionnaires would have helped us assess the preferences and motivations of the annotators, and enriched our user data.", " However, in striving for ecological validity this was a tradeof we chose to make, exemplifying tensions between eliciting real-world annotation with citizen science volunteers and gathering rich participant data.", " Generalizability to other sources of audio data.", " There are also limitations of our study due to our data.", " Our ground truth data were limited to a small selection of urban soundscapes which may reduce the generalizability of our results to other audio recordings.", " However, soundscapes such as these, dominated by technological and human sounds, are typically evaluated to be in the chaotic quadrant of Swedish Soundscape-Quality Protocol [2, 3].", " In a previous study [10], we found that as urban soundscapes become more complex, annotators\u2019 precision stays about the same but their recall goes up.", " Therefore, while future a study is necessary to assess how our current results will translate to other types of soundscapes or audio recordings, the results from our previous study provide some insight into how our current results may translate to other urban soundscapes.", " A general study of audio recording annotation would likely require a dataset several orders of magnitude greater in size and would not have been feasible in our study design.", " The ground-truth data were also limited due to the incompleteness of their labels.", " The efects of this can be seen in the surprisingly perfect precision results for the multi-label annotation task.", " With complete ground-truth labels, we suspect that the metrics would be lower but the trends would remain the same.", " Limitations aside, our analysis both supports and contrasts prior studies on the annotation of temporal media.", " We see how annotators \u201cover annotate\u201d in binary labeling tasks and", " \u201cunder annotate\u201d in multi-labeling tasks, similarly to [50]; and that errors often compound when an annotation task is broken down into a series of dependent sub-tasks, like [54].", " However, our fndings oppose [22] who suggested that a multi-labeling task resulted in lower annotator agreement and unhappier crowdworkers.", " In contrast, messages from our Zooniverse project\u2019s \u2018Talk\u2019 boards include comments from volunteers who found the binary task limiting; while no comments suggest that the multi-label task was too complex or time-consuming.", " There are too few messages to undertake a qualitative analysis, and so we include comments from three volunteers as illustration, and in order to spark ideas for future research (N.B.", " the animal diary and animal camera trap projects in the quotes are referring to Snapshot Serengeti [51] where volunteers are asked to identify wildlife from motion-triggered cameras left in the countryside):", " \u201cThere might be a better way than is that X sound yes or no to classify quicker.", " People will get tired of listening to sound clips faster than other quick options, like the animal diaries.", " You want to squeeze as much data out of each audio clip.\u201d", " \u201cI hear drums, observer/audience yelling applause, at least one large size dog that is very unhappy about the noise.", " This takes place outside.", " I have no way to label more than two features, so it will probably be more frustrating than I can deal with to participate.\u201d", " \u201cIn my opinion, this project should use the same model as the animal camera trap projects, that is, have a list of sound categories that one can click on for each clip, and give the opinion to choose more than one category.\u201d", " There are likely to be a range of other factors contributing to our fndings, each of which merits inquiry beyond the scope of this particular discussion.", " Recognition over recall.", " For example, having the full range of classifcation options in the multi-label task immediately visible and directly available may be signifcant, bringing to mind previous HCI discussion around the relative importance of recognition over recall with regards to direct manipulation interfaces (e.g., [9, 23, 49]).", " Motivations of volunteer citizen scientists.", " Another important factor may be the difering motivations of volunteer citizen scientists who are likely to be driven by intrinsic motivations [19, 40, 42], and paid crowd-workers who are primarily motivated by fnancial incentives [30, 37, 46].", " This may explain why binary labeling tasks, in which individual instances can be completed quickly, have appeared to be more efective in previous audio annotation undertaken by crowd-workers;", " and why, in contrast to this, our fndings suggest that multi- 7 ACKNOWLEDGMENTS labeling tasks may be more efective in the context of audio annotation with volunteer citizen scientists.", " Volunteers\u2019 contributions and accomplishments.", " A refective case study highlighting insights the Zooniverse UX team gained as the platform developed [52] further helps us to frame or fndings.", " For instance, audio tasks on the Zooniverse platform are considered less likely to sustain volunteers\u2019 participation than image-oriented tasks, highlighting the importance of designing workfows that maximize the impact of individual contributions.", " A second insight is that it is important for volunteer contributors to gain a sense of accomplishment, and in more monotonous tasks (such as ours) this should be achieved quickly.", " While on the surface the simple binary task is completed more quickly, it is possible that annotators will be presented with a succession of examples in which the sound in question is not present.", " If accomplishment is more closely associated with positively identifying sounds than negatively identifying their absence, it is likely that the binary task becomes demotivating, and that the multi-label task, which enables contributors to make a positive identifcation in every case, leads to earlier and more consistent feelings of accomplishment.", " A third insight is that volunteer discussion and collaboration, beyond the initial requirements of the task, has resulted in a number of citizen-led discoveries.", " This indicates that citizen scientists are not necessarily looking for simple tasks that can be completed as quickly as possible.", " Rather, they may be motivated to extend a task, investigate further, and gain a deeper understanding.", " Having the full range of classifcations options visible may prompt sharing and discussion, which are important to this process of social inquiry.", " However, one important caveat to this discussion is that our tasks required only a limited number of classifcation options.", " An increase in the number of categories from which labels are selected could quickly make multi-label tasks, in which all options were always visible, extremely challenging."], [" This paper contributes to our understanding of multi-label audio annotation crowdsourced with volunteer citizen scientists.", " We have described how a multi-labeling approach to annotation can result in annotations at a higher throughput and of comparable overall quality (as measured by F-score) to those obtained using binary-labeling, the technique most commonly used with paid crowdworkers.", " This supports previous work on image annotation with citizen scientists, and reminds us of the important diferences between participants who volunteer their time and efort freely, and paid crowdworkers, which we unpack in light of insights gained from crowdsourcing, citizen science, and HCI literature.", " We would like to thank all the Zooniverse volunteers who continue to contribute to our project.", " This work is supported by National Science Foundation award 1544753 (https://www.", " nsf.gov/awardsearch/showAward?AWD_ID=1544753)."]], "paperSentences": ["", "- Hello. Okay so yes, I'm Mark Cartwright from NYU, and I'll be talking about crowdsourcing multi-label audio annotation tasks with citizen scientists. So first of all context, so I'm a researcher  text wdsourcing Multi-label Audio Anno\u2019 ks with Citizen Sc Mark Cartwright, Graham Dove, Ana E Juan P. Bello, Oded Nov New York Unive Depa EE MARL sofive>)", "on the Sounds in New York City project, SONYC, which is a large project aimed at monitoring, analyzing, and mitigating urban noise pollution in New York City. Which has a lot of health and quality of life effects in the city.  text sofiro>)", "In particular I work in the machine listening crowdsourcing sides of the project and our goal is to build multi-label models that can detect multiple sound events in a scene to support our data-driven analysis and build tools for both citizens and city agencies.  text sofiro>))", "And we have deployed 60 sensors which have collected 130 million recordings that are 10 seconds long which equates to around 41 years worth of audio if you concatenate it all together. And this is the audio that we're gonna use to train our models but of course we need to label some of this audio. So in conjunction with the Department of  text sofiro>)) 130,000,000", "Environmental Protection in New York City and through inspecting the noise code we've developed a set of 23 fine level sound classes that are grouped into eight coarse level sound class categories for labeling our audio. And so this isn't really an exhaustive set of sounds but it's aimed to be descriptive and actionable while still being feasible for people to label and for us to train models on.  text", "So due to the public interest in this topic we've actually turned to citizen scientists to help us annotate some of this data. So we launched a campaign on Zooniverse which is the largest citizen science platform. But because of the temporal dimension of audio, audio annotation is much more time intensive than the image annotation tasks that really dominate Zooniverse. You can't just glance and click to annotate. So this had us question which annotation norms we should adopt on this task. And led us to the following research question  text", "which is how does the type of multi-label annotation task affect throughput and quality? Should we adopt the norms of paid crowdsourcing tasks, paid crowdsourcing audio tasks, rather, and break annotation into multiple binary annotation tasks, or should we adopt the norms of image annotation with citizen scientists like the tasks on Zooniverse and use multi-label annotation tasks? Therefore we started collecting our labels, started collecting labels of our sonic recordings on Zooniverse while simultaenously running a small study which aimed for high ecological validity. The study wasn't necessarily conducted as a controlled experiment but rather was an analysis of a real world data collection, you know solving real world audio annotation problems. We wanted authentic behaviors of volunteering citizen scientists which may differ from those of paid workers on commercial platforms. So to measure the quality of the labels we mixed in the occasional recording from YouTube for which we have some ground truth into our set of recordings for annotation. And we tested three task variations all of which gave us full multi-label annotations. We tested multiple paths of binary labeling, single pass multi-labeling, and somewhat hybrid multi-pass hierarchical multi-labeling. We had 339 participants and we asked five annotators per recording to annotate the multi-label tasks and for practical purposes we only collected three annotators per label for the binary but doing full multi-label annotation which I'll explain in a minute.  text joes the type of multi-label annotation tas 4 affect throughput and quality\u2019 + Do we adopt norms of paid crowdsourcing into multiple binary annotation ta: 1 do we adopt norms of 1\u00a2 annotation with c multi-label annotation ta", "So given this sound recording, (loud construction equipment) So for the binary labeling task in which it asks is there a jackhammer present in this recording let's say, I would say yes, there's a jackhammer present  text", "the high level classes such as impact tools  text \\nnotation Tas", "  text \\nnotation Tas", "So what do we find? So in regards to the annotation throughput we found that binary labeling generated more overall positive labels per recording. And the multi-label and hierarchical multi-label tasks generated about the same number of positive labels per recording. And as we'll see in a minute when there's more labels generated like in the binary labeling tasks this leads to higher recall in terms of the annotation quality which we'll see in a minute.  text Annotation Throughput + Binary labeling task generated more overall positive labels per recording YouTube Recordings Mutt-tobe! 1.25 Hrchl. Mutt-tabe! a oe 1 S31 ES + Mean number of generated labels per recording", "And also in regards to annotation throughput the binary labeling task, so the individual task, took half as long as the multi-label task for an individual annotation. Now a note about that is that we could do simply do positive and negative sampling of examples for binary annotation and not get the full multi-label annotation which you would have, let's say, we classifiers go through your data, find a, propose a list of recordings that it thinks are positive and then have those verified by annotators. However this requires good sampling mechanisms which is the case for some of the classes but many of the classes that is actually not the case. So in addition there's also been some studies that have shown that full multi-label annotations actually lead to better machine listening models as well.  text Annotation Throughput + Binary labeling task took hi ong \u00ab jabel for an individual annotation Time to Complete Individual Annotation Task Binary |} \u2014\u2014\u2014 jensen rch. Mi Hrehi. Me Time (s)", "doing binary annotation for all 23 classes of course that's gonna lead to a much longer time for getting complete, full multi-label annotation. And multi-label and hierarchical multi-label are much less in that sense.  text Annotation Throughput + However, for a full 23 class multi-label annotation binary labeling took 9x long as multi-labeling Time to Complete Full Multi-label Annotation =\u2014 = Multilabel_ Hrchl. Multi-tat Binary", "So in these plots here we have the f score on the top row, precision in the middle, and recall at the bottom, and multi-label annotation, then hierarchical and binary from left to right. And on the x-axis you have the minimum votes for a positive label. So let's say if only one annotator said that this class was present, it would get a positive label. And then the colors are the number of annotators that we asked to do the task. So here we find that volunteers tend to  text Annotation Quality", "So we were able to get some feedback from participants but due to the kind of constraints of the platform we weren't able to really survey all of the people so in the minimal feedback we were able to get we found that in general when given the binary labeling tasks the participants in the crowdsourcing platform felt underutilized. They wanted to do more but they couldn't and were frustrated. And this is a very different conclusion than people have come up with when doing similar tasks in paid crowdsourcing environments when given the multi-label task people actually in that task they actually become frustrated rather than the opposite. And they find it more overwhelming.  text Feedback from Participants (Binary Labelin + \u201cThere might be a better way than is that X sound yes or no to classify quicker. People will get tired of listening to sound clips faster than other quick options, like the animal diaries. You want to squeeze as much data out of each audio clip.\u201d 1 hear drums yelling applause, at least one large size dog that is very unhappy abs This takes place outside. | have no way to label more than two features, so it will probably be more frustrating than I can deal with to participate.\u201d In my opinion, this project should use the same model as the animal camera trap projects, that is, have a list of sound categories that one can click on for each clip, and give the opinion to choose more than one category.\u201d", "that overall quality of multi-label annotations from binary and multi-label tasks are pretty comparable, they have differences but they can be balanced through the way we aggregate the data. And multi-label annotation's much more efficient but only if you really need multi-label annotation. If you can get by with this kind of positive negative sampling, binary annotation can be efficient as well. In the hierarchical multi-label tends to propagate error leading to lower recall and in informal feedback that we received we indicated that volunteers much preferred the multi-label which the opposite of the paid crowdsource workers. And these results really side with the common practice of citizen science image annotation rather than that of paid audio crowdsourcing. So those sort of norms from the image annotation do go over to the audio annotation side as well.  text \u00bbnclusions of Study Overall quality of multi-label annotations from binary and multi-label tasks comparable. They have differences but they can be balanced. Multi-labet i h more efficient, but only if you need full multi-label annotation Hierarchical multi-label tends to propagates error, leading to lower recall Informal feedback indicates that volun\u2019 much preferred multi-label, opposite of paid crowdworkers Results side with the common practice of ence image annotation rather than that of paid audio crowdsourcing.", "So we've been continuing with this since we need to collect our data so we have an ongoing citizen science annotation campaign and thus far we've collected data from about 1,000 annotators, registered annotators rather, there's actually a couple thousand more unregistered annotators, and have received around 30,000 full multi-label annotations, audio annotations, and for around 10,000 completed audio recordings in which we got three annotators per recording. And our goal is to get about 1,000 positive examples  text 1,051 30,376 jegistered Full Multi-labet Annotator Annotatior", "the way data is distributed in actual natural recordings is very skewed towards these common classes so we're still fairly far from our goal. The green here is when we've gotten three annotators per label, they're the green in that label, and orange in two, blue one. So we still have a bit more work to do.  text Annotation Campaign", "We've actually released some of this data as part of a challenge which is part of the Detection and Classification of Audio, sorry, Acoustic Scenes and Events which we're hosting at NYU in October. So we released about close to 3,000 recordings. Some of these are training recordings and over 400 of them we annotated ourselves for validation and these have three Zooniverse annotators per recording.  text E 2019 Challenges Tz oustic scene classification \u00a9 audio tagging with noisy labels and minimal supervision und event localization and detection sound event detection in domestic environment Sound Tagging", "and compare it to how the crowdsourced annotators are comparing to the sonic research team they're doing pretty well on the coarse level but in the fine level there's still a lot of confusions. There's a lot more work that needs to be done to improve the quality of those. So we're looking into incorporating more experts into our loop to improve the quality of these fine level annotations.  text SONYC Urban Sound Tagging Dataset oniverse volunteers compare to those of the", ""], "evaluationData": {"boundariesAccuracy": 56.83, "timeAccuracy": 1.96, "structureAccuracy": 40.0, "mappingAccuracy": 92.76}}