•Human-centered computing Interaction paradigms; Gestural input; Activity centered design.
Clench Interface, Design Space, Clench Force Control
Xuhai Xu, Chun Yu, Anind K. Dey, Jennifer Mankoff. 2019. Clench Interaction: Novel Biting Input Techniques. In CHI Conference on
+ indicates the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI 2019, May 4–9, 2019, Glasgow, Scotland UK © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5970-2/19/05. . . $15.00 https://doi.org/10.1145/3290605.3300505
Human Factors in Computing Systems Proceedings (CHI 2019), May 4– 9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3290605.3300505
Modern ubiquitous interaction is increasingly being enriched by hands-free and eyes-free input from areas such as the face [3, 13], breath [27, 29], feet [20] and mouth [4, 26, 28]. In this paper, we contribute to that literature by expanding the expressiveness of mouth, specifically clenching. We eat every day. Biting is one of the most common and fundamental actions that we perform on a daily basis. Biting can be useful in situations where a user’s hands or eyes (or both) are busy, for augmenting immersive user experiences such as virtual and augmented reality (VR/AR), and for individuals with disabilities. For example, users can easily answer a phone call or control a music player when riding a bike. In addition to hands- and eyes-free properties, there are a number of reasons to leverage biting: it is common and natural hence it requires little training; it is subtle and therefore minimally distracting. Researchers have begun to develop mouth-based interactions, including tongue-based interfaces [28] and tooth-click interfaces [4, 40]. However, although tongue has been used for pointing [13, 26], and tooth clicking has been used for list selection and typing [4], there is value in exploring techniques for combining a range of spatio-temporal signals to support richer interactions, such as force and mouth region. In addition, a design space for interaction with the teeth needs to be developed. In this paper, we propose to use jaw clenching to create a jaw-based interface and explore a design space for clench input, which we define as a subset of biting in which the jaw is closed, but the force of pressure of the teeth against each other is changed. Jaw clenching is ideal for interaction because it can have many dimensions: force, length of time and location. The clench action is also subtle and low-impact, simply involving a squeezing of the jaw together. Clenching can support rich interactions since it can have multiple dimensions. Knowing how well users can control their clench force is fundamental for clench interaction design. We conducted a user study with a clench-controlled target acquisition task to
Paper 275 Page 1
evaluate users’ clench control ability. We compared different numbers of clenching force levels (N = 3, 4, 6 and 8) and three confirmation techniques (Quick Release, Time Dwell and Button Click). Our results show that users can easily discriminate N = 3 clench force levels and Quick Release outperforms other confirmation techniques.
We then developed three-dimensional design space (Force Level, Time and Location) for clench interactions. We deployed a survey to get user feedback on the design space and found the six clench actions in the design space most preferred by participants. We propose semantic interaction associations for the six clench actions and devise three interaction patterns for clench actions: clench gestures for discrete command input, clench for value control and clench integrated with other interactions.
We evaluated the usability of clench interaction from two perspectives. A second user study was conducted to measure the performance of the best six clench actions in virtual reality (VR). We also conducted a third user study to evaluate the usability of clench interaction in a situation close to a real use case, bicycling, where users’ hands and eyes are busy (as an example of both discrete command input and value control patterns). Clench interaction is faster than baselines in most cases in both studies, and participants stated a willingness to use clench as a novel input technique since it can be performed easily and conveniently. We use an in-mouth pressure sensor to obtain direct and accurate real-time clench force measurements, which we designed to maximize the realism by keeping the sensor thickness to 1.50mm. We measure isometric clenching with molar teeth, i.e., teeth and jaw do not move during interactions and the only variation is given by clench force (see Figure 1). While this sensor serves our goal of developing a robust design space for clench interaction, a deployable implementation can easily use other sensors based on our study results, which help to define parameters for design. For example, electromyography (EMG) is a viable alternative if force accuracy is unimportant (as is indicated by our design space survey), since there is a strong linear relationship between clench force and EMG signal on temporalis [5, 10]. And the positions of temporalis are compatible with modern devices such as VR/AR headsets and smart glasses. Our contributions in this paper are threefold:
• We proposed a novel approach to isometric clench interaction that leverages a clench on occlusal surfaces (see Figure 1) as a new input technique.
• We conducted a user study to demonstrate users’ ability to control clench force.
• We developed a design space for clench interaction and conducted two user studies that illustrated the high usability of the clench interaction.
We now review and summarize the related literature, focusing on face-related interaction, clench force measurement and clench detection in the wild.
There have been a variety of interaction techniques designed that leverage the face. For example, Salem and Zhai [26] proposed to use the tongue for isometric pointing tasks. Cheng et al. [9] proposed to detect whether a user applied pressure to the cheek using the tongue, as a gesture for hands-free interaction. Goel et al. [13] further expanded the input bandwidth by measuring different positions on one’s face touched by the tongue that support richer interactive tongue gestures. Sahni and Bedri et al. [6, 25] measured the movement of a small magnet glued to one’s tongue and deformation in the ear canal caused by jaw movement to support a silent speech interface. Facial expression is another input modality that has been investigated extensively. Lyons et al. proposed to use facial expressions as gestures for controlling a musical interface [18], and used mouth movement recognized by a camera to augment text-entry [17]. Ando et al. [2, 3] proposed to employ face-related movements as part of a hands-free music player and content reader. Much less work has focused on employing teeth as an input modality [4]. The tooth-click interface, developed as an assistive technology, is closely related to our work. Both Kuzume [15] and Zhong et al. [40] used an in-ear bone-conduction microphone to detect tooth click sounds as an additional input technique. Ashbrook et al. [4] investigated the click sounds of five different pairs of teeth to expand input bandwidth. A number of previous works used tooth-click for interactions such as selection [22, 40], confirmation [15, 39] and mode switch [28] with a single and/or double click. However, such an interface mainly relies on a instantaneous tooth click. It cannot leverage information such as clenching force, or sustained clench actions, etc. Clench interaction extends the input space along these dimensions.
Humans use different muscles for biting and chewing in different situations [14], leading to different properties; e.g., the
Paper 275 Page 2
maximum force varies greatly in different directions. The traditional instrument for directly measuring bite clench force is called a gnathodynamometer [19, 34]. In addition, there is some research that has studied the relationship between clench force and EMG signals on masseter and temporal anterior muscles of the face. Bakke et al. [5] found a strong linear correlation (p < 0.05, r = 0.98-0.99) between bite force and unilateral masseter or temporal muscle activity within the range of 12.5 to 87.5% of maximal unilateral bite force at eight sample points. The linear correlation becomes weaker when the bite force approaches the maximum. Bilt et al. [32] found the correlation between maximum voluntary bite force and total muscle activity ranged between 0.54 to 0.57. However, in previous research, all the devices were either on the outside of the face and quite inaccurate for continuous bite force measurement, or were in between the top and bottom teeth and had non-negligible height, mostly around 8 to 10 mm [5, 10, 32, 36]. This vertical height during biting can greatly affect muscular activity [19] and a gnathodynamometer is too thick for practical usage. Ideally the height should be nearly zero and close to the neutral condition of the mouth and teeth. This is a big reason to clenching, which is a subset of biting that involves pressing the teeth together with minimal jaw movements. Moreover, whether in or out of the mouth, none of the previous work has explored the human ability to control bite force except maximum bite force [32] and bite duration [31]. In this work, we investigated users’ ability to use clenching, to control clench force, and further proposed a set of clench interaction techniques that extend even beyond this clench force dimension.
Eating detection, which mainly involves chewing, biting and clenching, has been investigated in previous research. A number of methods have been proposed to detect jaw movement and mastication, such as acoustic sensors [1, 21, 23], inertial sensors [35], piezoelectric sensors [33] and proximity sensors [7, 16]. In addition, another common detection method is based on EMG signals. Zhang et al. designed 3-D printed glasses to detect chewing with EMG sensors placed near the temporal anterior muscles to detect muscular activity, and acceleration sensors to detect bone vibration [37]. They used their glasses for eating detection and achieved an F1 score of 0.770 with naturalistic data [38]. Blechert et al. [8] placed electrodes on a line between the mastoid and the masseter muscle to collect EMG signals and achieved an F1 score of 0.871 with in-the-wild data. These works show the potential for using EMG signals for multi-force-level clench interaction in the wild, especially as the detection of clenching/chewing can be easily
distinguished from other physical actions such as speaking, walking, and drinking [8, 35, 38]. Although we employed a pressure sensor in the mouth instead of EMG sensors to obtain an accurate measurement of clench force, we envision that an unobtrusive measuring system will not be an issue in the future.
We will now describe our first study, whose objective is to investigate human ability to control clench force for a basic discrete selection task with different degrees of visual feedback. The results of this study can guide clench interaction designs, such as the percentage of maximum force to be used for interaction, the number of levels of force that a user can easily distinguish, etc. We also compare three methods to find the best confirmation method for clench-based selection, since it is a fundamental aspect of any selection-based task.
A serial target acquisition and confirmation task was used to investigate the users’ ability to control their clench force. [24]. Based on the users’ sensed clench force, the cursor moved vertically on the screen, through regions. The number of regions was determined by the number of force levels being tested. If the user was able to confirm their selection while the cursor was in a specified region for a given trial, the trial was considered a success.
Clench force was uniformly mapped to its vertical distance after calibration for each user. The study included three different confirmation techniques and two visual feedback conditions. Before each trial, the user was asked to release the clench force and return to a neutral state. Confirmation techniques: We investigated three confirmation techniques, which are used after the cursor is in the intended region. Button Click (BC): pressing a button on the keyboard; Dwell (DW ): maintaining the cursor in the target region for 1s; and Quick Release (QR): quickly releasing the clench and moving the cursor below the pressure threshold (see Fig. 2). 300 ms was empirically chosen as the temporal
Paper 275 Page 3
threshold to recognize a “quick” release. The starting point of the releasing procedure indicates the selected position. Visual feedback conditions: Therewere two visual feedback conditions in our study. In the Full Feedback condition (FF ), regions are drawn on screen using black boxes, with the specified region for a trial in grey (see Figure 2). Correct cursor location is indicated by turning the specified region green. In the Partial Feedback condition (PF ), only the specified region is visible. There is no other regions for reference. The cursor will disappear once it moves across the threshold (the dotted line, see the right half of Figure 2).
We employed a within-subjects full factorial design with repeated measures. Four independent variables were included: visual feedback condition (FF, PF ), selection method (BC,DW, QR), the distance from the starting threshold to the target (D = 120, 305, 490, in pixel units, 576 is the maximum) and the number of clench force levels (N = 3, 4, 6, 8, corresponding target’s height H, are 192, 144, 96 and 72 in pixel units). We choose between 3 and 8 based on a pilot study involving three lab members. We found that 2 levels are very naive to distinguish and two is a limited number if we want to support rich interactions, while 10 levels are extremely difficult. In both feedback conditions, we used a Latin square to balance the appearance order of the selection method. D and N appeared in a randomized order. Participants repeated each condition three times. In order to have a balanced experiment, we took special cares when choosing D and H to appropriately distribute targets throughout the potential target space [24]. Figure 3 shows that D defines the target for a trial and indicates some location in the targets rather than the center of the target.
Since clench force varies across people and clenching positions, calibration is needed every time users put the sensor into the mouth. Relative clench force indicates the extent of how hard a user clenches. At the beginning of each session,
participants went through a calibration stage: a two-second relaxation to collect baseline and one isometric clench with the largest force to measure individual maximal clench force. Note that force of both sides of the mouth was recorded and the clench force used throughout Study 1 was the average of the two. In our study, the average maximum voluntary bite force was 189.2 N (SD = 65.2, Min = 97.8, Max = 290.1). We mapped 75% of the maximum force to the top of the interaction area to avoid fatigue from using an extreme position, based on the pilot study.
The dependent variables were: completion time: the time from when the cursor initially moved above the threshold until the acquisition was confirmed; success rate: the percentage of trials for a particular condition that resulted in successful acquisitions; number of crossings: the number of times the cursor cross the top or bottom edge of a target once the cursor has entered the target (minus 1 for all trials with QR); mean deviation: the average distance (relative to the height of the target) between the center of a target and the cursor position when the acquisition was confirmed. These measures complement each other. Completion time and success rate indicates the overall completeness of the tasks, while number of crossings and mean deviation reflects their force control performance.
Participants began with a warm-up stage. After they claimed that they understand the procedure, they went through two sessions in the order of FF and PF. A five-minute break was inserted between the sessions to relax their muscles. After each session, they filled out a simple questionnaire about their feeling of fatigue and success of the session and ranked the three selection methods according to their preference. Finally, the experimenter conducted a brief interview about participants’ reactions to using the clench interaction. The duration of the study was about forty minutes.
18 participants were recruited from a local university (Male = 11, Age = 22.3±1.7, ranging from 20 - 26). All participants reported having teeth in good condition and no previous disease or injury with their teeth.
We employed the thin-film pressure sensor TekScan Flexiforce (Model A201, Force range 0-440N) to accurately measure real-time clench force on occlusal surfaces. We used silicone sheets, steel flakes and rubber covers to protect the sensor. Figure 1 visualizes the layers. The diameter of the sensor was 15.00 mm. The thickness was 1.50 mm without pressure and reduced to 0.49 mm when clamped at 100 N
Paper 275 Page 4
force, which is the typical adult force’s during chewing [12]. All protective materials were sterilized and each participant used a new set of materials. During the study, two sensors were placed in users’ mouths to measure the clench force on both sides. The clench force sensors were connected to a laptop with Windows 10 through an Arduino UNO board. The experiment was conducted in Unity 2018.2.0.
The experiment resulted in 216 trials (2×3×4×3×3) for each participant and 4320 data points overall.
One of the goals of the user study was to find the best confirmation technique for clench interaction. We found a robust result: in most cases QR outperformed the other confirmation techniques in speed, success rate and clench force control, with statistical significance, in both feedback conditions. Moreover, QR was also ranked as the favorite by a majority of participants, in both visual feedback conditions (16 and 12, of 18 participants, respectively).
These results were confirmed by Repeated Measure Analysis of Variances (RM-ANOVAs) of the performance measures. Table 1 summarized the results, together with the results of post hoc pairwise t-tests with Bonferroni adjustment. Based on these results, we focus on the trials with QR in the rest of the statistical analysis.
The main goal of this study was to determine how many discrete levels of clench force users can discriminate comfortably and easily with decent performance. Not surprisingly, we found that fewer force levels led to better performance (see Figure 4). However, a small number of levels is not practical for interaction. Thus, our goal was to identify an appropriate maximum. Participant surveys suggest this number should be below 6 – they complained about the difficulty when N = 6 and 8 during the interview. “I felt very tired to finish the tasks when levels were more than four.” (P4). To identify an appropriate number of levels, we consider success rate. The rate were 92.3% for N = 3 levels, where N = 4 levels only had a 78.4%. Moreover, participants noticed this difference. They self-rated their own performance best when N = 3 on a 7-point Likert scale (1:strongly disagree - 7:strongly agree). Wilcoxon signed-rank post hoc tests showed significance, as summarized in Table 2. Therefore, we chose N = 3 as the “safe” number of discrete clench force levels that was easy for users to discriminate between.
Participant performance on all measures was better in the Full Feedback condition. RM-ANOVAs showed significant main effect of visual feedback on all four performance measures. With full visual feedback, participants spent less time (F1,17 = 6.66,p = 0.02), had higher success rate (F1,17 =
Measures
Full Feedback Partial Feedback AOV (F2,34) Post Hoc T-test AOV (F2,34) Post Hoc T-test
T 2.99,p = 0.063. QR < DW ∼ BC 18.04,p < 0.001∗∗∗ QR < DW < BC R 4.86,p = 0.014∗ QR ⪆ DW > BC 7.56,p = 0.002∗∗ DW > QR > BC C 20.98,p < 0.001∗∗∗ QR < BC < DW 25.47,p < 0.001∗∗∗ QR < BC < DW D 6.61,p = 0.003∗∗ QR < BC < DW 12.96,p < 0.001∗∗∗ QR ⪅ BC < DW
Table 1: Statistical Results of Confirmation Techniques. T : Completion Time, R: Success Rate, C: Number of Crossing, D: Mean Deviation. Symbol Explanation (the same below): . < 0.1, ∗ < 0.05, ∗∗ < 0.01, ∗∗∗ < 0.001. >/< implies significance. ⪅/⪆ indicates marginal significance. ∼means no significant difference.
Paper 275 Page 5
21.91,p < 0.001), fewer crossings (F1,17 = 7.99,p = 0.01), and less deviation (F1,17 = 25.37,p < 0.001).
Figure 5 reveals interesting trends when QR was the selection technique: participants made more mistakes (F2,34 = 3.97,p = 0.03) and had higher deviation (F2,34 = 10.88,p < 0.001) when using low clench force. This was also reflected in participants’ comments: 5 of 18 participants mentioned oversensitivity of the clench control system at low force level. “I was easy to overshoot when a target was low... I had a relatively better control when a target require larger clench force.” (P1)
Based on the results from Study 1, we identified a number of guidelines for clench interaction design: (1) Quick Release is the best technique for confirmation (2) N = 3 is a good number of clench force levels (3) Full visual feedback can facilitate interactions, especially
for multi-level force control. Study 1 mainly focused on force control, which is only one of the dimensions of clench interaction. There are other aspects to be explored in order to establish clench interaction techniques that can support a richer input space than the current state of the art in the literature. We developed a three-dimensional design space for clench interaction: • Force Level: Discretized Clench Force Three is a reasonable number of clench Force levels for users to distinguish easily and comfortably. Clench force at different levels can be mapped to different operations.
• Time: Repetition and Duration A clench can either be transient or sustained. For a transient clench, it can also be one-off or performed repeatedly. We propose four different points on the Time dimension: single, double and triple clench, as well as sustained clench.
• Location: Symmetry andAsymmetryHuman have the ability to control the left and right clench separately. Hence a clench on the Location dimension can either be performed symmetrically or asymmetrically. Table 3 shows the design space as well as some potential applications of each point in the design space.
Knowing which clench actions in the design space users are willing to use without considering the specific interactions they are mapped to is fundamental to understanding the potential usage of clench interaction. We conducted a questionnaire survey to evaluate our clench interaction design space.
We created a questionnaire to evaluate three aspects of each action of this new interaction technique on a 7-point Likert Scale (1 - not at all, 7 - very much): 1) Simplicity: the physical and mental ease of performing the action; 2) Convenience: the compatibility of the action with daily life; 3) Preference: the willingness to use the action in daily routines, without worrying about sensing.
We delivered the questionnaire to random pedestrians in the street at our local university. Participants were asked to perform each clench action in the design space several times and then rate this action. The evaluation order of the actions was shuffled to avoid sequential effect and fatigue bias.
Overall we delivered 64 questionnaires and 61 were completed (Male = 31, Age = 24.5±4.8, ranging from 19 to 40). Figure 6 shows the results of the average scores on the three aspects of each action in the design space. We ranked all actions according to the Preference score. Users preferred to use simple and easy clench actions. The top six actions have an average score over 4.5: symmetric single clench with level 1, symmetric single clench with level 2, symmetric double clench with level 1, symmetric sustained clench with level 1, asymmetric single clench with level 1, asymmetric sustained clench with level 1. This provides insightful guidance on interaction design: these six clench actions have the potential to be employed as basic actions for clench interaction, and the design of clench interactions should be compatible with these actions. Note that although higher complexity or physical demand of other clench actions may reduce users’ subjective preferences during the survey, there is a trade off between the sophistication of the interface and ability to complete tasks. We selected
Paper 275 Page 6
Single Double Triple Sustained
Simplicity Convenience Preference
L1 L2 L3
Single Double Triple
Sustained L1 L2 L3 L1 L2 L3
1
2
3
4
5
6
7 Sym
Asym
Figure 6: Heatmaps of The Evaluation Results of Clench Design Space. “L” indicates clench force level. The upper half are results for symmetric actions, while the lower half are results for asymmetric actions. For each single heatmap, the x-axis is for Force Level dimension and the y-axis is for Time dimension. Blue/white implies favorable/unfavorable.
the six actions to establish a basic set of clench interaction techniques, but we did not rule out the potential of the other actions to be added in future designs.
Clench Interaction Designs Based on the survey results, we propose a mapping of operations to the six clench actions, as summarized in Table 4. Note that we added the Symmetric Single Clench L3 into our design in order to have a complete force dimension. We then devised three interaction patterns where the advantages of clench could be fully leveraged: gesture for discrete command input, clench for value control, and clench integrated with other interactions such as pointing.
One of the most basic uses of clench actions is to interpret them as input commands. Clench holds the advantages of supporting hands-free and eyes-free interaction. Therefore, a user can issue a number of commands when both their hands or eyes are busy. One possible use case is when a user is riding a bicycle. Symmetric Single L1 Clench can be mapped to play/pause a music player (Click). Asymmetric Left/Right Single L1 Clench can be mapped to play the previous/next music (Discrete control). When a phone call comes in, Symmetric Single L1/2/3 clench can be mapped to accept/mute/reject an incoming phone call (Multiple Choice), and Symmetric Double L1 Clench can be used to hang up the call (Confirmation). Alternatively, clench gestures can also assist patients who may have difficulty moving their hands due to physical injuries, but retain the control of their face and mouth [13]. These gestures can be used as an assistive input method, e.g., menu selection [40], text entry [4] (Multiple Choice, Confirmation), smart home control (Click), etc. These examples show how clench gestures can improve interactions when one cannot fully use their hands or eyes.
In addition to discrete command inputs, clench can also be employed as a method to control discrete and continuous values without involving hands. Using the bicycling example again, when users enter a busy place where they may want to turn down the music volume to allow them to focus, or a quiet place where they might want to turn up the volume, Symmetric Single Clench with three force levels can be used to adjust the volume discretely, e.g., low, medium and high volume (Multiple Choices), or use Asymmetric Left/Right Sustained L1 Clench to continuously adjust the value in an isometric way (Continuous Control). These examples show how clench can be used for controlling a single value. Clench Integrated with Other Interactions. Clench can also function in an ecosystem with other input modalities, as an auxiliary input method to facilitate interaction. Integrating clench with other types of interactions such
Paper 275 Page 7
as pointing in virtual/augmented reality (VR/AR) can expand the input space. For example, users can use Symmetric Single L1 Clench as a simple click operation in VR/AR with a headpointing cursor to support basic hands-free interactions on a GUI (Click). Symmetric Sustained L1 Clench can be employed as a holding or clutching operation together with the head cursor to drag the thumb on a GUI scrollbar (Hold). Clench has an even richer space for game design in VR/AR, where clench gestures can be mapped to various game effects. For instance, in a first-person-shooter game, a Symmetric Single Clench with L1-3 can be mapped to different skills to defeat enemies (Multiple Choices), and a Symmetric Sustained Clench can be mapped to a skill as “becoming stronger temporarily” (Hold). When coordinated with other interactions, clench can be employed to enhance user experience.
From our design space evaluation, we obtained the six most preferred clench actions, however we still need to evaluate their usability. We conducted a second user study to evaluate their performance in supporting typical tasks. We compared the clench technique with two basic input methods: dwell and single button. We conducted the study on a VR platform where three types of clench interaction patterns and three techniques could be easily implemented. In this study, we mainly investigated scenarios where users are relatively static (i.e., sitting in a chair) to obtain initial results on a VR platform. Mobile scenarios are not involved. Note that we were focused on measuring the performance of clench interaction, rather than improving the results over the baselines.
Figure 7 shows the six tasks from the six actions in Table 4.
Task 1: Click - Symmetric Single L1 Clench. Users first need to move the head cursor (centered at the field of view) onto the target and use a Symmetric Single L1 Clench in clench method, click the button in button method, and keep the cursor on the target for 700 ms (same below) in dwell method (see Figure 7a). The next target will appear randomly, 500 pixels away.
Task 2: Confirmation - Symmetric Single/Double L1 Clench. Users are asked to judge the correctness of a simple formula (see Figure 7b). They use Symmetric Single or Double L1 Clench/single or double button click to indicate to whether it is correct or not in clench/button method, and move the cursor onto the green/red widget and stay in dwell method.
Task 3: Multiple Choice - Symmetric Single Clench at 3 Levels. Users have to choose the color for the white square to color it the same as the upper target square (see Figure 7c). Three Symmetric Single Clench force levels are mapped to the three colors: level 1, 2 and 3 to green, yellow and red respectively, for the clench method. Three color widgets are available for selection with the head cursor, and then a button click or dwell for the other conditions.
Task 4: Clutch - Symmetric Sustained L1 Clench. Users need to move the white target onto the blue square (see Figure 7d). Users move the cursor on the target and then use a Symmetric Sustained L1 Clench, or press down the button, or stay to pick up and hold the target. They can drop it by releasing the clench, releasing the button or staying on the blue square for the three techniques, respectively.
Task 5: Discrete Control - Asymmetric Single L1 Clench. A menu selection task is simulated. The blue square starts in the center, and users need to move it by five steps, each step being randomly chosen in the left or right direction, and then perform a confirm operation. Users use a left/right single L1 clench to move the blue square by one step and a Symmetric Single L1 Clench to confirm (clench). Three widgets are available to move the blue square (2 arrows) and to confirm the destination (square). The widgets can be selected with a button click for the button or using dwell when the cursor is on them.
Task 6: Continuous Control - Asymmetric Sustained L1 Clench. Users need to adjust the slidebar thumb to the provided value (see Figure 7f). Sustained Single Left/Right L1 Clench decreases/increases the slidebar value at a constant rate (clench). For button/dwell, two arrows are available and pressing down the button/staying on an arrow when the head cursor is on them will change the value at the same speed. A limitation when comparing gesture- and cursor-based techniques was that we did not vary the baselines, e.g., different button sizes and distances could lead to different results because cursor interaction is subject to Fitts’ law effects [11].
We use a within-subject study design to compare the three interaction techniques. The order of the six tasks are randomized for each subject. For each task, participants repeated three identical cycles. In each cycle, they use three techniques
Paper 275 Page 8
to finish the task 10 times, one technique after another. The order of the three techniques are counterbalanced to avoid a learning effect. Completion time was measured.
Participants were asked to keep the clench sensor in their mouth throughout the study in all three techniques for consistency. We used the same calibration procedure and mapping function as in Study 1. After each task, we asked participants to rank the three techniques according to their subjective preference. The experiment ended with a semistructured interview to gain additional feedback on clench interaction. The duration of the study was about 90 minutes.
12 participants were recruited from our local university (Male = 7, Age = 22.8±1.9, ranging from 20 to 28). All participants reported having good teeth condition and no previous disease or injury with their teeth.
We employed the same sensor in Study 1 tomeasure clench force. The study was run with an HTC Vive in Unity 2018.2.0.
We collected data from 90 trials (3× 3× 10) for each task. All participants learned the interactions easily during the warmup stage and our system recognized the actions with low error rate throughout the study (average F1 score equaled 0.965). Figure 8 shows the performance for the three techniques. Table 5 summarizes the statistical results of the comparison between clench interaction and the two baselines. We found that except for Task 1 and 2, clench was faster for all tasks, especially when the baselines’ input channel was not as rich as clench. We note that a different design for dwell or button might require less time than the designs in this paper. Our results still revealed the advantages of clench interaction in that it has a larger input space.
Other than Tasks 3 and 4, Friedman tests on the subjective rank data of all tasks showed significance. Nemenyi post-hoc tests on four tasks found that users mostly prefer clench over baselines (see the rank order column of Table 5). 4 out of 12 participants mentioned that clench has a stronger conceptual connectionwith one’s mind to use clench alone versus button click. “I prefer clench. When using button I have to consciously think about my hands.” (P7) This can be explained by the different conceptual mouth-head/hands-head distances.
The results in Study 2 revealed the feasibility of using clench interaction for controlled tasks. In Study 3, we are interested
in investigating the usability of clench interaction in a situation closer to daily use. We conducted a case study of simulated bicycling where users’ hands and eyes were busy and they had to perform tasks to control the music player on their phone and receive phone calls. We compared clench interaction with two baselines: interacting directly with the mobile phone or using a wireless bluetooth earphone.
In our simulated bicycling scenario, users need to perform three tasks with each method: answer/mute/reject a phone call (clench gestures for command input), switch to previous/next songs (discrete value control), and turn down/up the volume (continuous value control). For the clench interface, users use a symmetric single L1/2/3 clench for three reactions to a phone call, left/right single L1 clench for switching between songs, and left/right sustained L1 clench for adjusting the volume. We used default operations on the phone: buttons and sliders shown on the screen to react to calls, switch songs and adjust volume; earphone: single/double/triple click on center button for call reactions, double/triple click on center button for previous/next songs, and clicks on volume buttons for volume adjustment. We used a within-subjects design. The independent variable is the method of interaction: clench, phone, earphone. Each task was repeated 10 times. Each time the specific operation was randomly chosen and the order of tasks was counterbalanced. The time for completing each task with different methods was measured. Each participant was briefly interviewed at the end. The study lasted about 15 minutes.
Among the 12 participants in Study 2, 10 of them joined Study 3 (Male = 7, Age = 23.2±2.0, ranging from 20 to 28). Participants rode on an indoor exercise bike to simulate bicycling. An iPhone 8 (in the trouser pocket on side of the dominant hand) and Bose SoundSport Wireless Earphones were used to complete the tasks.
The average F1 score of recognizing clench actions were 0.973. As expected, the average time of executing operations using a clench interaction was much less than for the baseline interactions, i.e., clench interaction was the fastest. Figure 9 shows the time for each method in the three tasks. RM-ANOVA shows significance in all three tasks (F2,18 = 34.89, 21.99, 58.97, p < 0.001) and post hoc t-tests showed all pairs are significantly different. Users’ subjective comments also reflected the high usability of clench interaction when both hands and eyes are busy. “The clench obviously works better than other methods. I don’t have to reach my hand to the phone or earphone anymore.”
Paper 275 Page 9
Although the use of buttons has been a typical interaction paradigm for decades, clench might be able to serve as a more intuitive and convenient input channel. Participants in user studies mentioned a stronger conceptual connection with their mind from using clench alone versus button click (confirmation in Study 1 and interaction in Study 2). Although we did not rule out the possibility of a novelty effect, the different conceptual distances to the head might affect users’ mental model of clench interaction.
In order to make clench interaction applicable in daily use, clench actions need to be mapped to effects with reasonable semantics. For instance, a left/right single clench makes sense when mapped to the same conceptual direction as previous/next. Designers should make sure to connect the appropriate semantic meanings with clench actions as needed. In fact, another approach for a design space evaluation could be an elicitation study where users are asked to choose the action they prefer in the design space to complete a given task. This may provide a mapping set between clench actions and effects that better reflects users’ preference on specific tasks. This can be explored in the future.
Sensor. The thickness of the clench sensor is not completely negligible and it requires calibration each time the sensors are put in the mouth. Although we manufactured it as thin as 1.50 mm, it may still have some effect on the clench control in Study 1, and the user experience in Studies 2 and 3. Note that
we also tested an EMG sensor attached around the temporal muscles, with pressure sensors in the mouth simultaneously. Similar to previous work, we found that fatigue greatly effects the linear relationship between EMG and clench force [31] (r 2 = 0.803,p < 0.001 from a five-minute test of three pilot users). In the future, better signal processing methods with EMG might be able to fully reflect the clench force and thus obviate an in-mouth sensor.
Design. In Study 2, we only investigated the six most preferred clench actions in the design space. There are still a number of actions to be evaluated in future work, such as users’ ability to control clench asymmetrically and to use richer clenching patterns. In addition, there exist a number of potential modalities besides visual feedback. For example, Stewart et al. [30] investigated audio, and vibrotactile feedback and found that non-visual pressure input can be executed with similar speed but lower accuracy. Since clench interactions are essentially pressure-based, these modalities are worth further exploration when selection accuracy is not essential, especially in cases where visual feedback is not available.
In future work, we would like to test the device in the field. Although we tested its use on a stationary bike, this lacks the ecological validity of true mobile use.
Teeth health. We consulted two dentists and they pointed out the potential effect of teeth grinding if the maximum clench force is maintained too long. Although none of the participants ran into extreme fatigue or teeth grinding during our study, this is a limitation and needs further exploration.
In this paper, we propose clench interaction, which leverages isometric clench force. We first conducted a user study to investigate the ability to control clench force. Our results showed that three is a favorable number of force levels for users to distinguish easily and comfortably, and quick release outperforms other confirmation techniques. Based on the first user study, we developed a three-dimensional - Force Level, Time and Location - design space for clench interaction and evaluated it via a questionnaire survey, whose results helped select six user-preferred clench actions. We then devised three interaction patterns for clench interaction. We conducted a second user study to measure the performance
Paper 275 Page 10
of the six clench actions in controlled tasks, as well as a third user study to investigate the usability in a situation closer to daily use. Participants preferred the clench over the baselines and were willing to use clench as a novel interaction technique that can be performed easily and conveniently.
This work is supported by the National Key Research and Development Plan under Grant No. 2016YFB1001402, the Natural Science Foundation of China under Grant No. 61672314 and No. 61572276, Tsinghua University Research Funding No. 20151080408, and also by Beijing Key Lab of Networked Multimedia.
[1] Oliver Amft, Mathias Stäger, Paul Lukowicz, and Gerhard Tröster. 2005.
Analysis of Chewing Sounds for Dietary Monitoring. In Proceedings of the 7th International Conference on Ubiquitous Computing (UbiComp’05). Springer-Verlag, Berlin, Heidelberg, 56–72. https://doi.org/10.1007/ 11551201_4 [2] Toshiyuki Ando, Yuki Kubo, Buntarou Shizuki, and Shin Takahashi. 2017. CanalSense: Face-Related Movement Recognition System Based on Sensing Air Pressure in Ear Canals. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (UIST ’17). ACM, New York, NY, USA, 679–689. https://doi.org/10.1145/3126594. 3126649 [3] Toshiyuki Ando, Yuki Kubo, Buntarou Shizuki, and Shin Takahashi. 2018. CanalSense+: Face-Related Movement Recognition and Identification System Based on Air Pressure in Ear Canals. In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems (CHI EA ’18). ACM, New York, NY, USA, Article VS08, 1 pages. https://doi.org/10.1145/3170427.3186600 [4] Daniel Ashbrook, Carlos Tejada, Dhwanit Mehta, Anthony Jiminez, Goudam Muralitharam, Sangeeta Gajendra, and Ross Tallents. 2016. Bitey: An Exploration of Tooth Click Gestures for Hands-free User Interface Control. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’16). ACM, New York, NY, USA, 158–169. https: //doi.org/10.1145/2935334.2935389 [5] Merete Bakke, LARS MICHLER, KE Han, and Eigild Möller. 1989. Clinical significance of isometric bite force versus electrical activity in temporal and masseter muscles. European Journal of Oral Sciences 97, 6 (1989), 539–551. [6] A. Bedri, H. Sahni, P. Thukral, T. Starner, D. Byrd, P. Presti, G. Reyes, M. Ghovanloo, and Z. Guo. 2015. Toward Silent-Speech Control of Consumer Wearables. Computer 48, 10 (Oct 2015), 54–62. https: //doi.org/10.1109/MC.2015.310 [7] Abdelkareem Bedri, Apoorva Verlekar, Edison Thomaz, Valerie Avva, and Thad Starner. 2015. Detecting Mastication: A Wearable Approach. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction (ICMI ’15). ACM, New York, NY, USA, 247–250. https://doi.org/10.1145/2818346.2820767 [8] J Blechert, M Liedlgruber, A Lender, J Reichenberger, and FH Wilhelm. 2017. Unobtrusive electromyography-based eating detection in daily life: A new tool to address underreporting? Appetite 118 (2017), 168– 173. [9] Jingyuan Cheng, Ayano Okoso, Kai Kunze, Niels Henze, Albrecht Schmidt, Paul Lukowicz, and Koichi Kise. 2014. On the Tip of My
Tongue: A Non-invasive Pressure-based Tongue Interface. In Proceedings of the 5th Augmented Human International Conference (AH ’14). ACM, New York, NY, USA, Article 12, 4 pages. https://doi.org/10.1145/ 2582051.2582063 [10] Virgilio F Ferrario, Chiarella Sforza, Gianfranco Zanotti, and Gianluca M Tartaglia. 2004. Maximal bite forces in healthy young adults as predicted by surface electromyography. Journal of dentistry 32, 6 (2004), 451–457. [11] Paul M Fitts. 1954. The information capacity of the human motor system in controlling the amplitude of movement. Journal of experimental psychology 47, 6 (1954), 381. [12] Charles H Gibbs, Parker EMahan, Harry C Lundeen, Kenneth Brehnan, Edward KWalsh, andWilliam BHolbrook. 1981. Occlusal forces during chewing and swallowing as measured by sound transmission. Journal of Prosthetic Dentistry 46, 4 (1981), 443–449. [13] Mayank Goel, Chen Zhao, Ruth Vinisha, and Shwetak N. Patel. 2015. Tongue-in-Cheek: Using Wireless Signals to Enable Non-Intrusive and Flexible Facial Gestures Detection. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 255–258. https://doi.org/10.1145/2702123. 2702591 [14] JH Koolstra and TMGJ Van Eijden. 1995. Biomechanical analysis of jaw-closing movements. Journal of dental research 74, 9 (1995), 1564– 1570. [15] Koichi Kuzume. 2012. Evaluation of tooth-touch sound and expiration based mouse device for disabled persons. In Pervasive Computing and Communications Workshops (PERCOM Workshops), 2012 IEEE International Conference on. IEEE, 387–390. [16] Richard Li and Gabriel Reyes. 2018. Buccal: Low-cost Cheek Sensing for Inferring Continuous Jaw Motion in Mobile Virtual Reality. In Proceedings of the 2018 ACM International Symposium on Wearable Computers (ISWC ’18). ACM, New York, NY, USA, 180–183. https: //doi.org/10.1145/3267242.3267265 [17] Michael J. Lyons, Chi-Ho Chan, and Nobuji Tetsutani. 2004. MouthType: Text Entry by Hand and Mouth. In CHI ’04 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’04). ACM, NewYork, NY, USA, 1383–1386. https://doi.org/10.1145/985921.986070 [18] Michael J Lyons, Michael Haehnel, and Nobuji Tetsutani. 2001. The mouthesizer: a facial gesture musical interface. In Conference Abstracts, Siggraph 2001. 230. [19] Arturo Manns, Rodolfo Miralles, and Carmen Palazzi. 1979. EMG, bite force, and elongation of the masseter muscle under isometric voluntary contractions and variations of vertical dimension. The Journal of prosthetic dentistry 42, 6 (1979), 674–682. [20] Toni Pakkanen and Roope Raisamo. 2004. Appropriateness of Foot Interaction for Non-accurate Spatial Tasks. In CHI ’04 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’04). ACM, New York, NY, USA, 1123–1126. https://doi.org/10.1145/985921.986004 [21] V. Papapanagiotou, C. Diou, L. Zhou, J. van den Boer, M. Mars, and A. Delopoulos. 2017. A Novel Chewing Detection System Based on PPG, Audio, and Accelerometry. IEEE Journal of Biomedical and Health Informatics 21, 3 (May 2017), 607–618. https://doi.org/10.1109/JBHI. 2016.2625271 [22] Arthur Prochazka. 2005. Method and apparatus for controlling a device or process with vibrations generated by tooth clicks. US Patent 6,961,623. [23] Tauhidur Rahman, Alexander T. Adams, Mi Zhang, Erin Cherry, Bobby Zhou, Huaishu Peng, and Tanzeem Choudhury. 2014. BodyBeat: A Mobile System for Sensing Non-speech Body Sounds. In Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys ’14). ACM, New York, NY, USA, 2–13. https://doi.org/10.1145/2594368.2594386
Paper 275 Page 11
[24] Gonzalo Ramos, Matthew Boulos, and Ravin Balakrishnan. 2004. PressureWidgets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’04). ACM, New York, NY, USA, 487–494. https://doi.org/10.1145/985692.985754 [25] Himanshu Sahni, Abdelkareem Bedri, Gabriel Reyes, Pavleen Thukral, Zehua Guo, Thad Starner, and Maysam Ghovanloo. 2014. The Tongue and Ear Interface: A Wearable System for Silent Speech Recognition. In Proceedings of the 2014 ACM International Symposium on Wearable Computers (ISWC ’14). ACM, New York, NY, USA, 47–54. https://doi. org/10.1145/2634317.2634322 [26] Chris Salem and Shumin Zhai. 1997. An Isometric Tongue Pointing Device. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI ’97). ACM, New York, NY, USA, 538–539. https://doi.org/10.1145/258549.259021 [27] Gary P Scavone. 2003. The Pipe: explorations with breath control. In Proceedings of the 2003 conference on New interfaces for musical expression. National University of Singapore, 15–18. [28] Tyler Simpson, Michel Gauthier, and Arthur Prochazka. 2010. Evaluation of tooth-click triggering and speech recognition in assistive technology for computer access. Neurorehabilitation and neural repair 24, 2 (2010), 188–194. [29] Misha Sra, Xuhai Xu, and Pattie Maes. 2018. BreathVR: Leveraging Breathing As a Directly Controlled Interface for Virtual Reality Games. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Article 340, 12 pages. https://doi.org/10.1145/3173574.3173914 [30] Craig Stewart, Michael Rohs, Sven Kratz, and Georg Essl. 2010. Characteristics of Pressure-based Input for Mobile Devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’10). ACM, New York, NY, USA, 801–810. https://doi.org/10.1145/ 1753326.1753444 [31] Peter Svensson, A Burgaard, and S Schlosser. 2001. Fatigue and pain in human jaw muscles during a sustained, low-intensity clenching task. Archives of oral biology 46, 8 (2001), 773–777. [32] Andries Van Der Bilt, Anneke Tekamp, Hilbert Van Der Glas, and Jan Abbink. 2008. Bite force and electromyograpy during maximum
unilateral and bilateral clenching. European journal of oral sciences 116, 3 (2008), 217–222. [33] Tri Vu, Feng Lin, Nabil Alshurafa, and Wenyao Xu. 2017. Wearable Food IntakeMonitoring Technologies: A Comprehensive Review. Computers 6, 1 (2017). https://doi.org/10.3390/computers6010004 [34] Antti Waltimo and Mauno Könönen. 1993. A novel bite force recorder and maximal isometric bite force values for healthy young adults. European Journal of Oral Sciences 101, 3 (1993), 171–175. [35] Shuangquan Wang, Gang Zhou, Yongsen Ma, Lisha Hu, Zhenyu Chen, Yiqiang Chen, Hongyang Zhao, and Woosub Jung. 2018. Eating Detection and Chews Counting through Sensing Mastication Muscle Contraction. Smart Health (2018). [36] L Xu, S Fan, B Cai, Z Fang, and X Jiang. 2017. Influence of sustained submaximal clenching fatigue test on electromyographic activity and maximum voluntary bite forces in healthy subjects and patients with temporomandibular disorders. Journal of oral rehabilitation 44, 5 (2017), 340–346. [37] Rui Zhang and Oliver Amft. 2016. Bite Glasses: Measuring Chewing Using Emg and Bone Vibration in Smart Eyeglasses. In Proceedings of the 2016 ACM International Symposium on Wearable Computers (ISWC ’16). ACM, New York, NY, USA, 50–52. https://doi.org/10.1145/2971763. 2971799 [38] R. Zhang and O. Amft. 2018. Monitoring Chewing and Eating in FreeLiving Using Smart Eyeglasses. IEEE Journal of Biomedical and Health Informatics 22, 1 (Jan 2018), 23–32. https://doi.org/10.1109/JBHI.2017. 2698523 [39] Xiaoyu Amy Zhao, Elias D Guestrin, Dimitry Sayenko, Tyler Simpson, Michel Gauthier, and Milos R Popovic. 2012. Typing with eye-gaze and tooth-clicks. In Proceedings of the Symposium on Eye Tracking Research and Applications. ACM, 341–344. [40] Lin Zhong, Dania El-Daye, Brett Kaufman, Nick Tobaoda, Tamer Mohamed, and Michael Liebschner. 2007. OsteoConduct: Wireless bodyarea communication based on bone conduction. In Proceedings of the ICST 2nd international conference on Body area networks. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), 9.
Paper 275 Page 12
