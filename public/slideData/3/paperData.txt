• Human-centered computing → Interaction devices;
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5970-2/19/05. . . $15.00 https://doi.org/10.1145/3290605.3300556
Intraoral Interface, Hands-free, Input Device, Interaction Modality, Discreet Interaction, Refexive Interaction
Pablo Gallego Cascón, Denys J.C. Matthies, Sachith Muthukumarana, and Suranga Nanayakkara. 2019. ChewIt. An Intraoral Interface for Discreet Interactions. In CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA, 13 pages. https://doi.org/ 10.1145/3290605.3300556
Researchers have proposed a variety of hands-free interactions [5, 9, 77], contributing to a greater efciency in multitasking [82]. In addition, they have also proven to be useful when applied as assistive technologies [30, 34, 45]. This signifcantly helps people with physical impairments to regain essential interaction capabilities, in particular patients suffering from locked-in syndrome [69]. While these interfaces can be useful for people with special needs, they are usually cumbersome and disruptive [43, 46, 57, 63, 90]. Particularly these two limitations prevent healthy people without special needs from adopting these new interaction interfaces. In contrast, potentially socially acceptable technologies ofer only a very limited input bandwidth [54, 55]. We investigate a novel intraoral ("in-the-mouth") nonattached input interface, ChewIt, to strike a balance between social acceptability and a high input bandwidth. ChewIt
Paper 326 Page 1
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
provides discreet interactions, as users are able to hide the device inside the oral cavity. We exploit the well-pronounced dexterity of the tongue in conjunction with the jaw, teeth, and mouth cavity, to enable new input opportunities. To gain a better understanding of the peculiarities of discreetness and general feasibility, we initially ran a series of user studies (Study 1: spectator’s perception, S2: user’s perceived obstruction, S3: understanding habits and limitations). Based on the results of these evaluations, we continued investigating potential intraoral interface designs through a further series of studies (S4: implications on dimension, S5: comparing shapes, S6: volume factor, S7: defning gestures). Informed by these investigations, we developed a prototype system, ChewIt.
In summary, the main contributions of this paper include the design, implementation, and validation of a non-attached intraoral interface.
Oral Input Interfaces
Speech Interfaces: Systems which utilize speech interfaces, such as intelligent personal assistants [12] or cognitive assistants [21] are gaining popularity. Some examples include Siri [41], Alexa [2], and Cortana [11], which are embedded into smartphones, computers, and standalone devices. This input modality successfully provides assistance when the hands are occupied with a primary task, such as driving and making a call. However, its drawbacks are in creating disturbances for others [20] and compromising privacy concerns. Tackling this, Silent Speech Interfaces (SSIs) [17] are introduced aiming to maintain privacy, making speech interfaces more acceptable in public spaces. A great variety of diferent technologies [48] have been utilized, such as ultrasound [16, 18], video imaging [35–38, 76], audio [25], neuromuscular activity [43], electromyography [84], and electromagnetic motion [28, 85]. SSIs are also useful in environments where surrounding noise inhibits audible data, as well as benefcial for people with speech impairments [22, 29], who cannot use their own voice. However, current SSIs also yield a number of drawbacks. For instance, most interfaces are either intrusive [85], obtrusive [84], or require excessive processing times [37]. Apart from these technical issues, speech assistance yields a high memory burden and creates a disproportional cognitive load, for instance having to speak out a sentence when only a binary change is required [10].
Mouth Gesture Interfaces: Alternative mouth gesture interfaces have been introduced that enable a quick input mechanism, for instance controlling a computer with a simple sip and puf [62, 63]. Such interfaces rely on diverse technologies, such as gnathosonics [83, 86, 87] and audio to recognize the sound of tooth touches [47] to implement clicks
[4, 59]. Other technologies to detect mouth gestures include optical sensing [15, 31, 52], air pressure changes [3], microradar data [49], infrared [7], and EMG [90]. These interfaces provide a signifcant beneft for people with special needs, namely those experiencing limited mobility, and for applications that require of additional input [73]. However, as these interfaces are all externally attached, they are highly noticeable by others and thereby limit social acceptability among those not requiring special assistance.
To overcome these issues, semi-invasive approaches have been proposed, including a swallowed probe [32] and an in-ear-placed sensor [50, 53, 55]. In addition, invasive technologies, such as an RFID chip under the skin [33], piercings with magnets [39], and a tongue-glued magnet [70] have been investigated in research. As social acceptability of implantable technology may be questionable, we focus on semi-invasive technologies that can be easily removed by the users themselves. These semi-invasive mouth gesture interfaces are often used by people with physical impairments, such as Jouse [42] and IntegraMouse [40]. Most of these interfaces are comprised of two or more separate parts, in which one is placed inside the mouth [39, 46]. Although these interfaces enable great input mechanisms, most are either located on the palatal vault space [64, 71, 72, 74, 75, 81], at the buccal shelf area, or at the lower jaw [67], also creating speech impediments.
Social conventions [26] and context play an important role in the prevalence and acceptance [14, 27, 58] of novel technologies. As previously stated, using a speech assistant such as Siri [61] in public or in a meeting has the potential to create feelings of awkwardness or embarrassment for users. Keltner and Buswell [44] demonstrated that social embarrassment is a complex emotion. Several factors, such as loss of control and failure of privacy regulation, can make the early adoption of new user interfaces difcult [19, 68]. Rico and Brewster also aimed a part of their work [8] at exploring social acceptability, dividing it into two viewpoints: how individuals feel while interacting; and how others perceive the user interaction. Montero et al. [60] adopted this concept and refned these two viewpoints into "User’s Social Acceptance" and "Spectator’s Social Acceptance", which other researchers, such as Ahlstrom et al. [1], incorporated into their studies. This work focuses on an intraoral interface whereby interactions mimic an edible object, which is an acceptable behavior. When resting and not interacting, we envisage it to be invisible to others as users would be able to hide the interface at diferent places inside the mouth cavity. We conducted several studies to examine the acceptability of such an interface in terms of user perspective and spectator perspective.
Paper 326 Page 2
ChewIt is a novel intraoral interface, similar to an edible object, ofering new ways to perform hands-free interactions. ChewIt can be easily inserted and removed whenever the user pleases, as it is not attached to the mouth. ChewIt’s design enables the user to utilize several tongue and bite gestures that mimic the behavior of interacting with an edible object so that it does not draw increased attention of others. Furthermore, the device can be hidden in the mouth when not interacting, making it invisible to spectators. Based on the high level of dexterity and proprioceptive abilities of the tongue, once being familiar with the enabled input gestures, ChewIt can favour refexive interaction [56] potentially decreasing task load in multitasking scenarios.
ChewIt’s form factor and positioning inside the mouth provide a variety of unique features. In summary, the device:
• can be easily hidden in the mouth (see Figure 1b) and thus remains invisible when not interacting; • is ready to interact as soon as the user takes it from its location (see Figure 1c); • can be discreetly interacted with natural tongue gestures, as the device can be easily moved around, such as changing orientation, fipping, etc; • does not obstruct speech when not in use. In some instances, users were able to drink and eat while holding the interface in the mouth; • does not require specifc user adjustment or calibration as it follows a straightforward implementation; • enables hands-free and eyes-free interaction in a mobile context, ofering an additional interaction channel or being an assistive device for users with impairments.
Form: We used a polylactic acid flament (PLA) [23] to fabricate the casing according to the shape we determined based on our studies (see Figure 2). The electronics were
populated into a fexible PCB to enable a better ft into the 3D-printed casing. The dimensions of the fnal device are 30 × 16 × 7 mm, with a weight of approximately 3.92g. As the components of our prototype are not 100% bio-compatible, we covered the device with a thin latex layer. Additionally, we attached a cotton string to the device to prevent the user from swallowing it, although this use case is highly unlikely.
Hardware: The technology behind the ChewIt prototype is based on an Inertial Measurement Unit (IMU). We used an MPU-9250 [6], a 9-axis motion tracking device that combines a 3-axis gyroscope, a 3-axis accelerometer, a 3-axis magnetometer, and a Digital Motion Processor (DMP), which are all integrated into a small 3 × 3 × 1 mm IC. Using this IMU, we determine the orientation and movements of the device. In addition, we also placed a button inside the device, which can be triggered by a biting gesture. A 2.4GHz low-power SoC (nRF51822 [51]), embedding a microcontroller and Bluetooth transceiver, handles the data stream. The IMU and the microcontroller communicate via I2C protocol. The maximum bandwidth was 1.9KB/s. The button uses a simple GPIO interface. Currently, the average energy consumption of the device is approximately 10.6µA when in idle mode and 10mA when fully operating. The hardware is powered by a CR1220 coin-cell battery, which enables it to run for approximately three hours. To save power, the prototype is resting in a low-power mode waiting for an activation, such as bite.
Gesture Recognizer: To prove technical feasibility and to gain some initial user impressions, we implemented a gesture recognizer driving a machine learning approach by using a conservative feature engineering. Data Gathering: First, we recorded raw data from the accelerometer, gyroscope, and the button. We did not utilize the IMU’s magnetometer, since ChewIt should be invariant to the absolute orientation of the user. Although the IMU is capable of providing high sample rates, we sampled the data at 100Hz in order to reduce power consumption and possibly avoid jitter. As each gesture can be executed in less than 2.5 seconds, we selected a window size of 256 samples. For our training data set, we recorded a single window containing a single gesture. We selected 9 gestures + 1 default gesture, in which the user was continuing his daily routine. We did not include the "Swipe"-gesture, as it is based on the surface of the device. We recorded 42-90 repetitions of each gesture (class). These classes were recorded in static and dynamic conditions such as directing the head left, right, up, and down while sitting and walking. We recorded our test set on the next two days – fnally containing 31 repetitions for each class. This way, our collected test data set is spatially and temporally separated from the training set. We decided to do this to avoid strong over-ftting efects and thus to increase the stability of the model generated by the classifer.
Paper 326 Page 3
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
Feature Engineering: Because our gathered input data is rather low-dimensional, as the recorded repetitions are little, we have chosen a conservative feature engineering instead of utilizing a neural network approach. Therefore, for each window, we calculated 49 diferent features per input trajectory, as we also combined axes from the accelerometer and gyroscope. Thus, each class was described with 402 computed features. The attribute selection of a J48 DT implementation determined these attributes as the most meaningful ones:
meanCrossings(Button), minElement(Gx), rotationIndexFeature(Gx), frstQuartile(Gx), meanCrossingsAll, activityUnitAll, meanCrossings(Az), maxElement(Ay), maxElement(Gz), minElement(Gx), spectralLowHighBandQuotient(Ax), rotationIndexFeature(Ay), spectralEnergy (Gy), interQuartileRange(Az), frequencyDiferenceOfSecondAndThirdQuartile(Ax), rotationIndexFeature(Ay).
Classifer Selection: To evaluate which classifer would provide high performance, we undertook an empirical approach comparing a Support Vector Machine (SVM), Random Forest (RF), Bayes Net (BN), K-nearest neighbours classifer (IBk), and a computational inexpensive C4.5 Decision Tree (DT) by using a leave-kinstances-out method. Thereby, each instance represents a single repetition of an entire gesture. In total, the training set incorporated 617 instances, as the test set had a size of 310 instances. We built several models based on our training data set and tested the performance with our test data set. The results are displayed in Table 1. Comparing recall rates, a one-way ANOVA for correlated samples (F4,36 = 3.93, p<.01) revealed a statistical main efect. A Tukey’s HSD revealed that both, the SVM (M = 96.77%; SD = 6.86%) and the RF (M = 97.1%; SD = 3.86%), were performing signifcantly better than the DT (M = 86.13%; SD = 18%). When unwilling to compromise on accuracy, the RF may be the best choice here. However, when aiming to implement a computational inexpensive gesture recognizer, the DT seems to provide reasonable results. Performance Level: To determine where most confusions occurred, we included the confusion matrix of the DT in Figure 3. The visualization indicates that bite gestures ("Incisor Bite", "Molar Bite", and "Peripheral Bite") resulted in greater confusions compared to other tongue gestures. This is due to the limitations of our initial prototype, which only incorporated a single button. However, with the current prototype, we can achieve reasonable accuracy above 95% with the top fve gestures (+ default class: "No Gesture"). Considering this reduced set, we also implemented a real-time gesture recognizer using a DT and utilizing a sliding window approach, while shifting the window every 16 samples.
At the beginning of this research, we sought to understand the acceptance and general feasibility of an intraoral interface. Therefore, we conducted 3 user studies to investigate the spectator’s perspective when an object is placed in the mouth (study 1), the user’s perceived obstruction when pursuing daily tasks with an object in the mouth (study 2), and understanding user habits and limitations when using common intraoral objects, such as chewing gums (study 3).
Study 1: The Spectator’s Perspective We investigated the discreetness of an intraoral object, placed inside the mouth, from a spectator point of view.
Participants & Procedure: We used 2 diferent object sizes, small (25 × 15 × 7 mm, 1Kmm3) and large (30 × 18 × 7 mm, 3Kmm3), and recorded 2 users with the large object, the small object, and no object in the mouth. These recordings were taken for talking, smiling, and rotating the head from left to right. In summary, we created the following set of images/animations: (1) Talking: 3 videos (2 facial side views of 2 users, 2
facial front views of 2 users) (2) Smiling: 3 still images (2 facial side views of 2 users,
2 facial front views of 2 users) (3) Rotating Head: 3 videos (1 view each from 2 users)
These viewing conditions were distributed as an online survey. Participants were instructed to point the images where they could spot the devices. In case of doubt, they were asked to not make a selection. The survey was completed by 42 participants, from which 14 were female, with an age range of 20 to 55 years.
Paper 326 Page 4
Confdence: Although we took pictures under diferent light conditions, an ANOVA did not evidence the light condition to have a main efect on the participants’ confdence level for Condition 1 (F3,154 = 0.155, p>.05) and Condition 2 (F3,154 = 0.214, p>.05). We also ran a t-test for Condition 3, which also did not show any statistical diference (T (76) = 0.507, p>.05). For the majority of answers, the participants were not fully confdent in identifying the image/animation which depicted an object in the mouth (M=63.66%; SD=1.75). In only M=3.15% (SD=0.56) of all cases, participants stated to be confdent in identifying the object.
Accuracy: When drawing attention to the fact that there was a hidden device (see Table 2), the participants could correctly identify the larger size in M=36.4% (SD=6.3%) of all the cases and the smaller size to a percentage of M=22.05% (SD=1.61%). Answers that were mistaken are M=18.05% (SD=3.64%). In M=23.5% (SD=2.53%) of all cases, the spectator could not detect anything. In conclusion, we found that spectators hardly noticed a chewing interface, when no interaction occurred and when the interface merely rested inside the buccal cavity. As two correct answers were included among the three images, there was a higher possibility that participants would guess with greater accuracy. However, the data appears almost normally distributed. Incorrect answers were slightly higher than 33%, as some participants did not spot any device in their responses. In terms of confdence, we can also confrm that most participants did not feel fully confdent while trying to spot the devices, once again evidencing that the device is hardly noticeable when not interacting.
Study 2: User’s Perspective In this study, our goal was to determine the level of obstruction a user may perceive when carrying an intraoral object.
Participants & Procedure: We recruited 10 participants (3 females, 7 males) and provided them with a rectangular object (10 × 20 × 5 mm) produced from a Class IIa long-term bio-compatible resin [24]. We asked them to hold the object in their mouth between the inner cheek and the teeth (Molar or Premolar). Participants were required to perform daily
ofce activities for 60 minutes, including writing and typing, etc, during this time we had conversations with them for 30 minutes and observed their behaviour throughout. After the experiment, we asked participants to rate the perceived obstruction on a 5-point Likert scale (1: not obstructing at all, 5: absolutely obstructing).
Low Self-Perceived Obstruction: On average, participants did not feel disturbed with an object being placed in the mouth (M = 2.1/5; SD = 0.57). P6: "I almost forgot having the device inside the mouth before you started talking to me." Two participants changed the location of the object because they were irritated by the constant pressure against the mouth tissue. All participants were still capable of clearly articulating themselves without noticeable diference. Some participants were even able to drink a beverage and consume a snack with the object in their mouth.
Study 3: Understanding Habits and Limitations In this study we aimed to investigate user habits when chewing a piece of gum.
Participants & Procedure: We recruited 14 participants (5 females, 9 males) and provided them with a piece of regular chewing gum. The task was to chew it until their jaws experienced fatigue. Once they fnished chewing, we asked participants to hide the gum in their mouth. Additionally, we asked them to point out the area in which they were holding the gum in their mouth (see Figure 1b: Molar <Blue>, Premolar <Yellow>, Cuspid <Red>, Incisor <Green>).
Chewing Time: The minimum chewing time until a participant decided to stop chewing was 17.35 minutes (M = 41.86; SD = 18.69). Two users chew the gum for more than one hour, indicating not being bothered to continue.
Holding Location: All but one participant used the Molar or Premolar teeth for chewing. All participants either held the chewing gum between those teeth or in the inner cheek, as the inner cheek was the preferred location.
The design of our prototype was informed by four studies, in which we focused on diferent aspects such as Dimensions, Form Factor and Gesture Feasibility.
Study 4: Implications on Dimension In this study, we sought to evaluate possible dimensions that would be most comfortable for the user and questioned whether size afects basic physiological activities.
Participants & Procedure: We recruited 12 participants (2 females, 10 males), asking them to hold objects of several sizes between the inner cheek and teeth. The objects were
Paper 326 Page 5
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
3D printed, using the same bio-compatible resin from the previous study. We started with a rectangular-shaped object (10 × 20 × 5 mm) and incrementally expanded its size in one dimension. First, we increased the width by increments of 5mm. Participants had to rate the clarity of speech with a 5-point Likert scale and they chose the size that they were most comfortable with. Second, the chosen object was then expanded in length by an increment of 5mm. Participants had to rate the impact of the object on their facial expressions using a 5-point Likert scale and chose the size that they are most comfortable with. Lastly, the thickness was expanded by an increment of 5mm and participants were asked to rate "pufness" of the face with a 5-point Likert scale and chose the one they felt was most comfortable (see Figure 4).
Size: All participants found a width of 10mm to be comfortable. Only 6 out of 13 participants found the subsequent size of 15mm to be comfortable. A length of 20mm was reported to be comfortable by all participants. Only 6 out of 13 participants found the subsequent size of 25mm to be comfortable. Thickness of 5mm was found to be comfortable to all participants. Only 1 participant reported the next size of 10mm to be comfortable. Clarity of Speech: Statistical diferences for (1) clarity of speech (Q = 23.52, p<.05, DF = 36) are confrmed by a Friedman test (k=3). A post-hoc analysis using Nemenyi’s procedure (k=3) two-tailed test found a signifcant diference (p<.05) between size A (M= 4.5; SD= 0.54) and size B (M= 3.62; SD= 0.62) and between size A and size C (M= 3.04; SD= 0.78). There was no statistical diference between B and C.
Impact of Facial Expressions: A Friedman test indicated a statistical diference of (2) facial expressiveness (Q = 37.74, p<.05, DF = 36). A post-hoc analysis using Nemenyi’s procedure (k=3) two-tailed test found a signifcant diference (p<.05) between size A (M= 4.35; SD= 0.72) and size E (M= 1.808; SD= 0.88). There was no statistical diference between D (M= 3.04; SD= 0.75) and E.
Impact of Facial ‘Pufness’: Wilcoxon’s test (k=3) indicated that self-perception of the "pufness" of the face (V = 88.5,
p<.05) was signifcantly diferent between size A (M= 4; SD= 0.89) and size F (M= 2.15; SD= 0.8). Based on the results, we found the size of 10mm (width), 20mm (length), and 5mm (thickness) to be acceptable.
Study 5: Implications of Shape In this study, we investigate how users perceive diferent geometrical shapes in terms of comfort, orientation recognition, and the ease of maneuvering it within the mouth.
Participants & Procedure: We recruited 12 participants (3 females, 9 males) to interact with 4 diferent shapes. These were (1) Asymmetrical Spherical Wedge (2) Spherical Cut, (3) Rectangular Prism, and (4) Triangular Prism (see Figure 5d). Building from the results of the previous study, we selected the dimensions 10 × 20 × 5 mm, All of them had a volume of 1Kmm3 and similar proportions.
We asked participants to orient and rotate the object along pitch-, roll-, yaw-axis (see Figure 1d) and rate the comfort, understandability of the object’s orientation, and ease of maneuverability of it on a 5-point Likert scale (1:worst, 5:best).
Orientation: Friedman’s test (k=4), indicated a statistical diference (Q = 32.09, p<.05, DF = 44) between the shapes in terms of orientation 5.a. A post-hoc analysis using Nemenyi’s procedure (k=3) two-tailed test found signifcant diference (p<.05) between Asymmetrical Spherical Wedge (M= 4.5; SD= 0.74) and Rectangular Prism (M= 2.53; SD= 0.65), between Spherical Cut (M= 4.5; SD= 0.74) and Rectangular Prism and between Triangular Prism (M= 3.7; SD= 1.22) and Rectangular Prism. There was no statistical diference (p>.05) between Triangular Prism and Spherical Cut.
Maneuverability: Friedman’s test (k=4), indicated a statistical diference (Q = 29.17, p<.05, DF = 44) between the shapes in terms of maneuverability 5.b. A post-hoc analysis using Nemenyi’s procedure (k=3) two-tailed test found signifcant diference (p<.05) between Rectangular Prism (M= 3.1; SD= 0.85) and Asymmetrical Spherical Wedge (M= 1.63; SD= 0.87) and between Rectangular Prism and Triangular Prism (M= 2.3; SD= 0.89). There was no diference (p>.05) between Triangular Prism and Spherical Cut (M= 2.9; SD= 1.2).
Comfort: Friedman’s test (k=4), indicated a statistical difference (Q = 12.9, p<.05, DF = 44) between the shapes in terms of comfort. A post-hoc analysis using Nemenyi’s procedure (k=3) two-tailed test found signifcant diference (p<.05) between Asymmetrical Spherical Wedge (M= 3.17; SD= 1.03) and Rectangular Prism (M= 1.75; SD= 0.965) and between Spherical Cut (M= 3.17; SD= 0.93) and Rectangular Prism. There was no statistical diference (p>.05) between Triangular Prism (M= 1.92; SD= 0.79) and Spherical Cut. Participants commented that fat surfaces are more comfortable
Paper 326 Page 6
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK
than round surfaces when placed against the teeth. However, round surfaces were reportedly more comfortable against the cheek compared to fat surfaces. This occurs because of the anatomical features of the mouth. Users frequently reported that when placing a round surface against a non-fexible tissue, such as the teeth, the shape applies pressure onto a small area, which does not allow the object to be stable nor comfortable. However, as the inner cheek is a fexible tissue, having a round shape against it enables a better grip and increases comfort. Therefore, we avoided corners and sharp edges, as they are uncomfortable and may even cut the soft mouth tissue.
Holding Location: The Molar area is preferred (Asymmetrical Spherical Wedge: All participants; Spherical Cut: 11 out of 12 participants; Triangular Prism: 11 out f 12 participants; Rectangle: 9 out of 12 participants), and as such the device will be designed to ft inside the Molar cavity (see Figure 1b).
Shape of ChewIt: We followed a systematic process to generate a reasonable shape for ChewIt. ChewIt’s shape underwent various transformations. Based on the rectangle dimensions extracted from the previous study, we removed sharp corners (see Figure 6b). This was transformed to an asymmetric shape with a uniform weight distribution. These features are important to understand orientation and maneuverability (see Figure 6c). ChewIt also transformed to have a fat surface on one side and a rounded surface on the opposite, allowing it to be held with the inner cheek and to sit comfortably in the teeth (see Figure 6d).
Study 6: Volume Factor In this study, we evaluate the impact of diferent volume proportions in terms of comfort and self-perceived discreetness.
Participants & Procedure: We recruited 13 participants (4 females, 9 males), who were given six objects with diferent volumes: (A) 0.25Kmm3, (B) 0.5Kmm3, (C) 0.75Kmm3, (D) 1Kmm3, (E) 2Kmm3, and (F) 3Kmm3. These were administered one at a time, and participants were asked to orient the object in diferent ways inside the mouth. The order of the objects was randomized.
Participants were asked to rate the maneuverability when they were orienting the device using a 5-point Likert scale (1: very easy, 5: very difcult). Participants were also asked to hide the object at two locations (see Figure 1b): Location 2 (at the Bottom, blue) and Location 1 (at the Top, black). After testing each location, participants were asked to indicate their preference for each location based on self-perceived comfort and discreetness.
Shape: To analyze the diferences in maneuverability, we ran a one-way ANOVA, which confrmed a signifcant main efect (F5,72 = 3.594, p<.05) between sizes (see Figure 7). A post-hoc analysis using Tukey’s HSD test revealed statistical diferences between Size D (M=4.58; SD=0.73) and Size A (M=3.3; SD=1.39) and between Size E (M=4.52; SD=0.64) and
Paper 326 Page 7
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
Size A. Size A was the size with the lowest mean score. There were no other statistical diferences (p>.05). Size D and Size E also had the higher mean score. Therefore, we chose a size between Size D and E as a reasonable size. In conclusion, we found smaller shapes to be easier to maneuver but excessively small ones were difcult to orient. However, the largest sizes were found easy to orient but could become cumbersome to move.
Holding Location: We did not fnd a preferred location (see
Figure 1b) in terms of comfort, as 8 out of 13 participants chose Location 1 (at the Top, Black) and 5 out of 13 participants chose Location 2 (at the Bottom, Blue). However, we observed that all participants, in terms of discreetness, chose Location 2 as the most preferable position for hiding the object. As we prioritize the discreetness of the interface, the devices will be optimized to ft best into Location 2.
Study 7: Defining Gestures Finally, we explore potential gestures that users could use to interact with an intraoral interface.
Participants & Procedure: We recruited 11 participants (3 females, 8 males) to study 14 gestures. The gestures are divided into 5 main groups (see Table 3). (a) Bites: performed by pressing the object with the teeth from opposite sides. We study the bites performed on the periphery of the object’s surface (Peripheral Bite) and the bites performed on the center of the surface, using the Incisors and the Molars teeth (see Figure 8a); (b) Rotations: performed on Roll, Pitch, and Yaw axis (see Figure 8b); (c) Tongue Movements: performed by placing the object on the top of the tongue and moving it from the left to the right or from the front to the back (see Figure 8c); (d) Location Changes: performed by moving the object from one place of the mouth to another (Figure 8d); (e) Tongue Drawing: performed by using the tip of the tongue to draw either Swipes or Complex Drawings, such as circles, triangles, or squares (see Figure 8e).
To help the participant understand the gestures, the experimenter demonstrated them with a 3D-printed replica of a mouth. After performing each gesture, the participants were asked to rate them based on (1) Ease of Use, (2) Natural Look, (3) Comfort, (4) Unobtrusiveness. The ratings were completed using a 5-point Likert scale (1:worst, 5:best). The order in which the gestures were performed in were randomized. Also, to judge the ‘natural look’, we placed a mirror in front of the participant.
Gesture Feasibility: Our evaluation revealed that gestures have individual strengths and weaknesses (see Figure 9), depending on the observed parameter. While some gestures, such as Complex Drawings and Tongue: Middle to Front are hardly noticeable, they are difcult to perform. Across all the ratings, we found that the gestures Pitching as well as Location Middle to Front are less preferred. A one-way ANOVA showed a statistical main efect across all ratings: Ease of Use (F13,181 = 7.84, p<.05), Natural Look (F13,181 = 4.7, p<.05), Comfort (F13,181 = 6.22, p<.05) and Unobtrusiveness (F13,181 = 4.38, p<.05). The post-hoc analysis is accomplished using a Tukey’s HSD to determine detailed signifcances.
In order to discriminate a subset for each parameter, we followed two criteria: 1) Gestures need to have a strong interpretation (i.e., mean Likert ratings in the top Quartile), and 2) The chosen gesture(s) need to be statistically diferent from as many non-strong gestures as possible.
Comfort: The subset deduced for this parameter involves two gestures: Rolling (M= 4.54; SD= 0.52) and Molar Bite(M= 4.46; SD= 0.66).
Ease of Use: The subset deduced for this parameter involves two gestures: Molar Bite (M= 4.53; SD= 0.52) and Rolling (M= 4.46; SD= 0.66).
Paper 326 Page 8
1 2 3 4 5
Molar Bite
Rolling
Peripheral Bite
Incisor Bite
Swipes
Translation: Left-Right
Location Middle-Side
Location: Side to Side
Translation: Front-Back
Location: Bottom to Top
Complex
Yawning
Pitching
Location: Middle to Front
Natural Look: . The subset deduced for this parameter involves ten gestures: Molar Bite (M= 4.31; SD= 1.33) , Translation: Left-Right (M= 4.15; SD= 0.69), Peripheral Bite (M= 4.15; SD= 0.8), Location: Side to Side (M= 4.08; SD= 0.95), Rolling (M= 4.08; SD= 1.12), Location: Middle to Side (M= 4; SD= 1,08), Translation: Front-Back (M= 3.77; SD= 1.09), Location: Bottom to Top (M= 3.62; SD= 1.04), Incisor Bite (M= 3.54; SD= 1.33) and Swipes (M= 3.46; SD= 1.33).
Unobtrusiveness: The subset deduced for this parameter involves three gestures: Incisor Bite (M= 4.38; SD= 0.96), Molar Bite (M= 4.15; SD= 0.9) and Rolling (M= 4.15; SD= 0.8).
Based on the users’ ratings on 4 parameters (Unobtrusiveness, Comfort, Natural Look and Ease of Use) we identifed two gestures that stood out from the rest: Rolling and Molar Bite. However, in case the scenario requires a larger input bandwidth, the range of gestures can increase up to ten without afecting the discreetness of the interactions.
The main contribution of this paper is the idea of an intraoral interface, which is similar to an interactive edible object. Such an interface will not only beneft impaired people, but also users in daily hands-busy situations, particularly when performing high precision tasks that require both hands [13]. Furthermore, we contribute with our design decisions and fndings on Interaction time, Holding Location, Shape Considerations, Comfort on Shape, Dimensions and Volume Considerations, and Gesture Feasibility.
Minimum Interaction Time: Our studies revealed that participants felt comfortable with holding a small object in their mouth for at least 15 minutes. This was assessed while the distraction continued with their daily tasks.
Two Holding Locations: We identifed two locations (see
Figure 1b) in which the users are able to hide an introal interface. The frst location (at the Top, Black) is the buccal shelf on the Maxilla, next to the Zigomatic Bone, partially under the Masseter Muscle, and the second is at the Body of the Lower Jaw Bone, under the Molar area (at the Bottom, Blue). With the frst location, the device seems to be less visible to others.
Two Basic Shape Considerations: We investigated 4 types of shapes, which are an Asymmetrical Spherical Wedge, a spherical cut, a rectangular prism, and a triangular prism. We developed 2 conclusions from such fndings: Among those shapes, we found asymmetry to be an important factor in understanding the orientation of the device; a fat surface on one side and a rounded surface on the opposite maximizes the grip and the comfort. In future, we aim to specifcally explore into texture perception and weight distribution.
Comfort on Shape: While performing the studies, we discovered that users mostly preferred rounded corners. Sharp corners should be avoided at all cost, as there is a high chance of cutting the soft tissues inside the mouth. This fact is refected by the comments from users, suggesting that pressure exerted by the small corners were irritating and annoying.
Dimensions and Volume Considerations: The dimensions of an object placed inside the buccal cavity can afect the clarity of speech, facial expression, and the self-perception of the face. Smaller sizes seem to be easier to maneuver inside the mouth. Where the size is excessively small, users will fnd it difcult to orient. Likewise, larger sizes are easy to orient but cumbersome to maneuver.
Self-perceived and Spectator-perceived discreetness: Users sufer from a subjective efect when wearing diferent devices, regardless of the size. This could be due to the sensation of having something inside the mouth. However, this was
Paper 326 Page 9
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
hardly noticeable by spectators in the user’s vicinity, even with the largest object size.
Gesture Feasibility: We propose 2 gestures that stand out across all the studied parameters: Rolling and Molar Bite. However, there are 8 more gestures that can be used if a particular application needs larger input bandwidth, without compromising the discreetness.
Multitasking. ChewIt allows for multitasking, including gestures with low levels of obstruction (see Figure 9), which will enable users to perform basic tasks, such as speaking while wearing the interface. We observed that some users were even able to drink and eat while having ChewIt in the mouth. However, we do not recommend this, as there is a high risk of swallowing it. In future, we aim to specifcally explore how users experience drinking, eating, and other physiological activities while wearing the device, and also to fnd implementation alternatives that could beneft the user in case the device is swallowed.
Although ChewIt enables new opportunities for future HCI, it inevitably has certain limitations, as with every other technology.
Hygienic Aspects: Concerns regarding hygiene and sterilization are evident when placing devices inside the mouth or spitting them out. Sharing ChewIt may therefore be inappropriate or unacceptable by users.
Safety Concerns: A major concern in our studies was to avoid any type of bacterial infection risk. Therefore, we used a bio-compatible resin that can be sterilized with alcohol after every use and can be 3D printed by the FormLabs2 Printer. When deploying ChewIt as a product, a crucial factor is the type of battery used. Silver oxide batteries are a possible option. Although they are not biocompatible, silver oxide batteries are used in colonoscopy cameras [79, 80] and are still safer than Lithium batteries. Also, using electronic components which are absolutely biocompatible proves challenging.
Material Properties: While the current prototype is based on a rigid material, it is desirable to use a fexible material. This material must withstand several conditions, including being biocompatible, resistant to bacteria, and stand high forces of extensive biting. We recommend the usage of a bio-compatible silicon compound.
Prototype: The current prototype has an 86.14% of overall accuracy within all 10 possible gestures. We plan to increase the accuracy by implementing existing solutions into the next prototype iterations. These solutions take into account the orientation of the object and the inertia when walking
by including: 1) a secondary IMU [65], 2) taking into account existing inertia [88] and the pendulum-like behaviour when walking [89], 3) the orientation of the device [78], and 4) a pressure matrix that detects bites on diferent regions within the surface of ChewIt. However, real-world scenarios do not demand such a high bandwidth of gestures for controlling conventional interfaces. We recommend a subset of 2 gestures: Rolling and Molar Bite for general purpose applications, such as controlling a music interface, controlling a wheelchair, or navigating through menus while engaging in another activity. As mentioned before, the accuracy of the current prototype for a subset of 2 gestures is 94.98%.
Social Acceptability : Using a discreet intraoral interface may be a novel way to interact hands-free. We evaluated social acceptance based on whether a spectator could detect that a user is holding ChewIt, as well as if a user’s perceived obstruction. However, social acceptability is a complex emotion [44] that depends factors such as context, individual preferences, and culture. While our initial evaluation indicates that ChewIt ofers a variety of discreet gestures, their social acceptance remains untested.
In this paper, we presented ChewIt, an intraoral interface that enables hands-free input operations. Our goal was to strike a better balance between social acceptability and expanding capabilities beyond merely providing a binary input. We studied social acceptance from a spectator’s point of view, as well as self-perception, while participants used intraoral objects. We developed a prototype where design decisions were derived by a series of user studies. We view ChewIt as a novel input interface that can assist people with and without impairments. For future work, we need to investigate ChewIt in public spaces, including among ethnic groups, to gain greater insights on the evolvement of user acceptance during longterm usage. In technical terms, one may consider using Electromagnetic Articulograph (EMA) to further determine the device’s accuracy when performing input gestures or positioning tasks. Related research also indicates that nonattached intraoral devices, such as electric chewing gums [66], could be a new interaction modality infltrating the user’s body in the future.
This work was supported by Assistive Augmentation research grant under the Entrepreneurial Universities (EU) initiative of New Zealand. We would like to thank all the participants for taking part in our user studies and anonymous reviewers for their valuable comments and helpful suggestions.
Paper 326 Page 10
[1] David Ahlström, Khalad Hasan, and Pourang Irani. 2014. Are you
comfortable doing that?: Acceptance studies of around-device gestures in and for public settings. In Proceedings of the 16th international conference on Human-computer interaction with mobile devices & services. ACM, 193–202. [2] Amazon Alexa. 2014. Retrieved August 15, 2018 from https://developer. amazon.com/alexa. [3] Toshiyuki Ando, Yuki Kubo, Buntarou Shizuki, and Shin Takahashi. 2017. CanalSense: Face-Related Movement Recognition System based on Sensing Air Pressure in Ear Canals. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology. ACM, 679– 689. [4] Daniel Ashbrook, Carlos Tejada, Dhwanit Mehta, Anthony Jiminez, Goudam Muralitharam, Sangeeta Gajendra, and Ross Tallents. 2016. Bitey: An exploration of tooth click gestures for hands-free user interface control. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services. ACM, 158–169. [5] Sharmila Ashok. 2017. High-level hands-free control of wheelchair–a review. Journal of medical engineering & technology 41, 1 (2017), 46–64. [6] 9 axis MotionTracking Inertial Measurement Unit MPU-9250. https://www.invensense.com/products/motion-tracking/9-axis/mpu9250/. [7] Abdelkareem Bedri, David Byrd, Peter Presti, Himanshu Sahni, Zehua Gue, and Thad Starner. 2015. Stick it in your ear: Building an inear jaw movement sensor. In Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers. ACM, 1333–1338. [8] Stephen Brewster, Roderick Murray-Smith, Andrew Crossan, Yolanda Vasquez-Alvarez, and Julie Rico. 2009. The gaime project: Gestural and auditory interactions for mobile environments. British computer Society (2009). [9] Wei-Hung Chen. 2015. Blowatch: Blowable and hands-free interaction for smartwatches. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on human factors in computing systems. ACM, 103– 108. [10] Malcolm J Cook, Charles Cranmer, Robert Finan, Andy Sapeluk, and Carol-Ann Milton. 2017. 15 Memory load and task interference: hidden usability issues in speech interfaces. Engineering Psychology and Cognitive Ergonomics: Volume 1: Transportation Systems (2017). [11] Microsoft Cortana. 2015. Retrieved August 15, 2018 from https: //www.microsoft.com/en-us/cortana. [12] Benjamin R Cowan, Nadia Pantidi, David Coyle, Kellie Morrissey, Peter Clarke, Sara Al-Shehri, David Earley, and Natasha Bandeira. 2017. What can i help you with?: infrequent users’ experiences of intelligent personal assistants. In Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services. ACM, 43. [13] The da Vinci Surgical System. Retrieved August 28, 2018 from http://www.davincisurgery.com/da-vinci-surgery/ da-vinci-surgical-system/. [14] Darren W Dahl, Rajesh V Manchanda, and Jennifer J Argo. 2001. Embarrassment in consumer purchase: The roles of social presence and purchase familiarity. Journal of consumer research 28, 3 (2001), 473– 481. [15] Piotr Dalka and Andrzej Czyzewski. 2010. Human-Computer Interface Based on Visual Lip Movement and Gesture Recognition. IJCSA 7, 3 (2010), 124–139. [16] Bruce Denby, Yacine Oussar, Gérard Dreyfus, and Maureen Stone. 2006. Prospects for a silent speech interface using ultrasound imaging. In
Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, Vol. 1. IEEE, I–I. [17] Bruce Denby, Thomas Schultz, Kiyoshi Honda, Thomas Hueber, Jim M Gilbert, and Jonathan S Brumberg. 2010. Silent speech interfaces. Speech Communication 52, 4 (2010), 270–287. [18] Bruce Denby and Maureen Stone. 2004. Speech synthesis from real time ultrasound images of the tongue. In Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP’04). IEEE International Conference on, Vol. 1. IEEE, I–685. [19] Sebastian Deterding, Andrés Lucero, Jussi Holopainen, Chulhong Min, Adrian Cheok, Annika Waern, and Stefen Walz. 2015. Embarrassing interactions. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 2365–2368. [20] Aarthi Easwara Moorthy and Kim-Phuong L Vu. 2015. Privacy concerns for use of voice activated personal assistant in the public space. International Journal of Human-Computer Interaction 31, 4 (2015), 307– 335. [21] Maria R Ebling. 2016. Can cognitive assistants disappear? IEEE Pervasive Computing 15, 3 (2016), 4–6. [22] Michael J Fagan, Stephen R Ell, James M Gilbert, E Sarrazin, and Peter M Chapman. 2008. Development of a (silent) speech recognition system for patients following laryngectomy. Medical engineering & physics 30, 4 (2008), 419–425. [23] Ultimaker Polylactic Acid Filament(PLA). Retrieved August 30, 2018 from https://ultimaker.com/en/products/materials/pla. [24] FormLabs. Dental LT Clear Resin. Retrieved August 30, 2018 from https://formlabs.com/materials/dentistry/. [25] Masaaki Fukumoto. 2018. SilentVoice: Unnoticeable Voice Input by Ingressive Speech. In The 31st Annual ACM Symposium on User Interface Software and Technology. ACM, 237–246. [26] Margaret Gilbert. 1983. Notes on the concept of a social convention. New Literary History 14, 2 (1983), 225–251. [27] Erving Gofman. 1956. Embarrassment and social organization. American Journal of sociology 62, 3 (1956), 264–271. [28] Jose A Gonzalez, Lam A Cheah, Jie Bai, Stephen R Ell, James M Gilbert, Roger K Moore, and Phil D Green. 2014. Analysis of phonetic similarity in a silent speech interface based on permanent magnetic articulography. In Fifteenth Annual Conference of the International Speech Communication Association. [29] JA Gonzalez Lopez, Lam A Cheah, Phil D Green, James M Gilbert, Stephen R Ell, Roger K Moore, and Ed Holdsworth. 2017. Evaluation of a silent speech interface based on magnetic sensing and deep learning for a phonetically rich vocabulary. In Proceedings of the Annual Conference of the International Speech Communication Association, Interspeech. ISCA, 3986–3990. [30] Tauseef Gulrez, Alessandro Tognetti, Woon Jong Yoon, Manolya Kavakli, and John-John Cabibihan. 2016. A hands-free interface for controlling virtual electric-powered wheelchairs. International Journal of Advanced Robotic Systems 13, 2 (2016), 49. [31] Gaurav Gupta. 2018. Improved Hands-Free Text Entry System. (2018). [32] Ryo Hayashi, Kazuhiro Tsuga, Ryuji Hosokawa, Mitsuyoshi Yoshida,
Yuuji Sato, and Yasumasa Akagawa. 2002. A novel handy probe for tongue pressure measurement. International Journal of Prosthodontics 15, 4 (2002). [33] Kayla J Hefernan, Frank Vetere, Lauren M Britton, Bryan Semaan, and Thecla Schiphorst. 2016. Insertable digital devices: voluntarily under the skin. In Proceedings of the 2016 ACM Conference Companion Publication on Designing Interactive Systems. ACM, 85–88. [34] Marco Hirsch, Jingyuan Cheng, Attila Reiss, Mathias Sundholm, Paul Lukowicz, and Oliver Amft. 2014. Hands-free gesture control with a capacitive textile neckband. In Proceedings of the 2014 ACM International
Paper 326 Page 11
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK P. Gallego Cascón et al.
Symposium on Wearable Computers. ACM, 55–58. [35] Thomas Hueber, Elie-Laurent Benaroya, Gérard Chollet, Bruce Denby,
Gérard Dreyfus, and Maureen Stone. 2010. Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips. Speech Communication 52, 4 (2010), 288–300. [36] Thomas Hueber, Gérard Chollet, Bruce Denby, Gérard Dreyfus, and Maureen Stone. 2007. Continuous-speech phone recognition from ultrasound and optical images of the tongue and lips. In Eighth Annual Conference of the International Speech Communication Association. [37] Thomas Hueber, Gérard Chollet, Bruce Denby, Gérard Dreyfus, and Maureen Stone. 2008. Phone recognition from ultrasound and optical video sequences for a silent speech interface. In Ninth Annual Conference of the International Speech Communication Association. [38] Thomas Hueber, Gérard Chollet, Bruce Denby, and Maureen Stone. 2008. Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application. Proc. of ISSP (2008), 365–369. [39] Xueliang Huo, Jia Wang, and Maysam Ghovanloo. 2008. A magnetoinductive sensor based wireless tongue-computer interface. IEEE transactions on neural systems and rehabilitation engineering 16, 5 (2008), 497–504. [40] IntegraMouse+. 2011. Retrieved August 22, 2018 from https://www. integramouse.com/en/home/. [41] Apple iOS Siri. 2010. Retrieved August 15, 2018 from https://www. apple.com/ios/siri/. [42] Jouse3. Retrieved August 28, 2018 from http://www.compusult.net/ web/guest/assistive-technology/our-at-products/jouse3. [43] Arnav Kapur, Shreyas Kapur, and Pattie Maes. 2018. AlterEgo: A Personalized Wearable Silent Speech Interface. In 23rd International Conference on Intelligent User Interfaces. ACM, 43–53. [44] Dacher Keltner and Brenda N Buswell. 1997. Embarrassment: its distinct form and appeasement functions. Psychological bulletin 122, 3 (1997), 250. [45] Hyeonseok Kim, Heon Jeong, and Jaehyo Kim. 2016. Feasibility of Hands-Free Smart-Device Interface for Quadriplegic Patients. International Information Institute (Tokyo). Information 19, 9A (2016), 3835. [46] Jeonghee Kim, Hangue Park, Joy Bruce, Erica Sutton, Diane Rowles, Deborah Pucci, Jaimee Holbrook, Julia Minocha, Beatrice Nardone, Dennis West, et al. 2013. The tongue enables computer and wheelchair control for people with spinal cord injury. Science translational medicine 5, 213 (2013), 213ra166–213ra166. [47] Koichi Kuzume. 2010. Input device for disabled persons using expiration and tooth-touch sound signals. In Proceedings of the 2010 ACM Symposium on Applied Computing. ACM, 1159–1164. [48] Wenshi Li. 2016. Silent speech interface design methodology and case study. Chinese Journal of Electronics 25, 1 (2016), 88–92. [49] Zheng Li, Ryan Robucci, Nilanjan Banerjee, and Chintan Patel. 2015. Tongue-n-cheek: non-contact tongue gesture recognition. In Proceedings of the 14th International Conference on Information Processing in Sensor Networks. ACM, 95–105. [50] Roman Lissermann, Jochen Huber, Aristotelis Hadjakos, Suranga Nanayakkara, and Max Mühlhäuser. 2014. EarPut: Augmenting Earworn Devices for Ear-based Interaction. In Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI ’14). ACM, New York, NY, USA, 300–307. https://doi.org/10.1145/2686612.2686655 [51] Bluetooth low-energy transceiver nRF51822. https://www.nordicsemi.com/eng/Products/Bluetooth-lowenergy/nRF51822. [52] Michael J Lyons, Michael Haehnel, and Nobuji Tetsutani. 2003. Designing, playing, and performing with a vision-based mouth interface. In Proceedings of the 2003 conference on New interfaces for musical expression. National University of Singapore, 116–121.
[53] Denys JC Matthies. 2013. InEar BioFeedController: a headset for handsfree and eyes-free interaction with mobile devices. In CHI’13 Extended Abstracts on Human Factors in Computing Systems. ACM, 1293–1298. [54] Denys JC Matthies, Simon T Perrault, Bodo Urban, and Shengdong Zhao. 2015. Botential: Localizing on-body gestures by measuring electrical signatures on the human skin. In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services. ACM, 207–216. [55] Denys JC Matthies, Bernhard A Strecker, and Bodo Urban. 2017. Earfeldsensing: a novel in-ear electric feld sensing to enrich wearable gesture input through facial expressions. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 1911–1922. [56] Denys J. C. Matthies. 2017. Refexive Interaction - Extending Peripheral Interaction by Augmenting Humans. PhD Thesis. University of Rostock. [57] Jess McIntosh, Asier Marzo, and Mike Fraser. 2017. SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Refection. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology. ACM, 593–597. [58] Rowland S Miller. 1995. On the nature of embarrassabllity: Shyness, social evaluation, and social skill. Journal of personality 63, 2 (1995), 315–339. [59] Tamer Mohamed and Lin Zhong. 2006. Teethclick: Input with teeth clacks. Technical Report. Technical Report. Rice University. [60] Calkin S Montero, Jason Alexander, Mark T Marshall, and Sriram Subramanian. 2010. Would you do that?: understanding social acceptance of gestural interfaces. In Proceedings of the 12th international conference on Human computer interaction with mobile devices and services. ACM, 275–278. [61] Aarthi Easwara Moorthy. 2013. Voice activated personal assistant: Privacy concerns in the public space. California State University, Long Beach. [62] Imad Mougharbel, Racha El-Hajj, Houda Ghamlouch, and Eric Monacelli. 2013. Comparative study on diferent adaptation approaches concerning a sip and puf controller for a powered wheelchair. In Science and Information Conference (SAI), 2013. IEEE, 597–603. [63] Sip n-Puf from Origin Instruments. Retrieved August 24, 2018 from http://www.orin.com. [64] Lotte NS Andreasen Struijk, Eugen R Lontis, Michael Gaihede, Hector A Caltenco, Morten Enemark Lund, Henrik Schioeler, and Bo Bentsen. 2017. Development and functional demonstration of a wireless intraoral inductive tongue computer interface for severely disabled persons. Disability and Rehabilitation: Assistive Technology 12, 6 (2017), 631–640. [65] Georg Ogris, Paul Lukowicz, Thomas Stiefmeier, and Gerhard Tröster. 2012. Continuous activity recognition in a maintenance scenario: combining motion sensors and ultrasonic hands tracking. Pattern analysis and applications 15, 1 (2012), 87–111. [66] Naoshi Ooba, Kazuma Aoyama, Hiromi Nakamura, and Homei and Miyashita. 2018. Unlimited Electric Gum: A Piezo-based Electric Taste Apparatus Activated by Chewing. In Adjunct Prublication of the 31st ACM User Interface Software and Technology Symposium (UIST ’18. ACM. [67] Hangue Park and Maysam Ghovanloo. 2014. An arch-shaped intraoral tongue drive system with built-in tongue-computer interfacing SoC. Sensors 14, 11 (2014), 21565–21587. [68] Thomas Ricker. 2016. First Click: When new tech is just too embarrassing to use. Retrieved August 28, 2018 from https://www.theverge. com/2016/11/2/13496330/embarrassing-technology. [69] Marie-Christine Rousseau, Karine Baumstarck, Marine Alessandrini, Véronique Blandin, Thierry Billette de Villemeur, and Pascal Auquier. 2015. Quality of life in patients with locked-in syndrome: Evolution over a 6-year period. Orphanet journal of rare diseases 10, 1 (2015), 88.
Paper 326 Page 12
[70] Himanshu Sahni, Abdelkareem Bedri, Gabriel Reyes, Pavleen Thukral, Zehua Guo, Thad Starner, and Maysam Ghovanloo. 2014. The tongue and ear interface: a wearable system for silent speech recognition. In Proceedings of the 2014 ACM International Symposium on Wearable Computers. ACM, 47–54. [71] Chris Salem and Shumin Zhai. 1997. An isometric tongue pointing device. In Proceedings of the ACM SIGCHI Conference on Human factors in computing systems. ACM, 538–539. [72] T Scott Saponas, Daniel Kelly, Babak A Parviz, and Desney S Tan. 2009. Optically sensing tongue gestures for computer input. In Proceedings of the 22nd annual ACM symposium on User interface software and technology. ACM, 177–180. [73] Ronit Slyper, Jill Lehman, Jodi Forlizzi, and Jessica Hodgins. 2011. A tongue input device for creating conversations. In Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 117–126. [74] Lotte NS Andreasen Struijk. 2006. An inductive tongue computer interface for control of computers and assistive devices. IEEE Transactions on biomedical Engineering 53, 12 (2006), 2594–2597. [75] Lotte NS Andreasen Struijk, Line Lindhardt Egsgaard, Romulus Lontis, Michael Gaihede, and Bo Bentsen. 2017. Wireless intraoral tongue control of an assistive robotic arm for individuals with tetraplegia. Journal of neuroengineering and rehabilitation 14, 1 (2017), 110. [76] Ke Sun, Chun Yu, and Lanand Shi Yuanchun Shi, Weinanand Liu. 2018. Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands. In Proceedings of the 31st ACM User Interface Software and Technology Symposium (UIST ’18). ACM. [77] Li Sun, Souvik Sen, Dimitrios Koutsonikolas, and Kyu-Han Kim. 2015. Widraw: Enabling hands-free drawing in the air on commodity wif devices. In Proceedings of the 21st Annual International Conference on Mobile Computing and Networking. ACM, 77–89. [78] Lin Sun, Daqing Zhang, Bin Li, Bin Guo, and Shijian Li. 2010. Activity recognition on an accelerometer embedded mobile phone with varying positions and orientations. In International conference on ubiquitous intelligence and computing. Springer, 548–562. [79] Paul Swain. 2008. The future of wireless capsule endoscopy. World journal of gastroenterology: WJG 14, 26 (2008), 4142.
CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK
[80] Medtronic Pillcam Colon 2 System. Retrieved August 20, 2018 from http://www.medtronic.com/covidien/en-us/products/ capsule-endoscopy/pillcam-colon-2-system.html. [81] Hui Tang and David J Beebe. 2006. An oral tactile interface for blind navigation. IEEE Transactions on Neural Systems and Rehabilitation Engineering 14, 1 (2006), 116–123. [82] Kathryn G Tippey, Elayaraj Sivaraj, and Thomas K Ferris. 2017. Driving while interacting with Google Glass: Investigating the combined efect of head-up display and hands-free input on driving safety and multitask performance. Human factors 59, 4 (2017), 671–688. [83] KW Tyson. 1998. Monitoring the state of the occlusion–gnathosonics can be reliable. Journal of oral rehabilitation 25, 5 (1998), 395–402. [84] Michael Wand, Christopher Schulte, Matthias Janke, and Tanja Schultz. 2013. Array-based Electromyographic Silent Speech Interface. In BIOSIGNALS. 89–96. [85] Jun Wang, Ashok Samal, and Jordan R Green. 2014. Preliminary test of a real-time, interactive silent speech interface based on electromagnetic articulograph. (2014). [86] David M Watt. 1966. Clinical applications of gnathosonics. Journal of Prosthetic Dentistry 16, 1 (1966), 83–95. [87] David M Watt. 1966. Gnathosonics - a study of sounds produced by the masticatory mechanism. Journal of Prosthetic Dentistry 16, 1 (1966), 73–82. [88] Jing Yang, Eun-Seok Choi, Wook Chang, Won-Chul Bang, Sung-Jung Cho, Jong-Koo Oh, Joon-Kee Cho, and Dong-Yoon Kim. 2004. A novel hand gesture input device based on inertial sensing technique. In Industrial Electronics Society, 2004. IECON 2004. 30th Annual Conference of IEEE, Vol. 3. IEEE, 2786–2791. [89] Shuozhi Yang and Qingguo Li. 2012. IMU-based ambulatory walking speed estimation in constrained treadmill and overground walking. Computer methods in biomechanics and biomedical engineering 15, 3 (2012), 313–322. [90] Qiao Zhang, Shyamnath Gollakota, Ben Taskar, and Raj PN Rao. 2014. Non-intrusive tongue machine interface. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 2555–2558.
Paper 326 Page 13
